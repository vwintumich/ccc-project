{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stage 2: Embedding Generation with BGE-M3\n\n**Primary author:** Victoria\n**Builds on:**\n- *Hierarchical_Clustering_Indicators_with_BGE_M3_Embeddings.ipynb* (Victoria/Sahana — BGE-M3 model selection and inline embedding approach)\n- *NC_Comprehensive_Embeddings.ipynb* (Nathan — multi-model comparison that helped justify BGE-M3 as the primary model)\n- `01_data_cleaning.ipynb` (Stage 1 output: verified unique indicators)\n\n**Prompt engineering:** Victoria\n**AI assistance:** Claude (Anthropic)\n**Environment:** Great Lakes (GPU required) or Google Colab (GPU enabled)\n\nThis notebook loads the deduplicated list of 12,622 verified unique indicator strings\nproduced by Stage 1, generates 1024-dimensional embeddings using the BGE-M3 sentence\ntransformer model, and saves the results as `.npy` and `.csv` files for downstream\ndimensionality reduction (Stage 3) and clustering (Stage 4).\n\n**Great Lakes session settings:**\n- Partition: gpu\n- GPUs: 1 (V100 or A40)\n- CPUs: 4\n- Memory: 32GB\n- Wall time: 1 hour (embedding takes ~2-5 min; most time is model download on first run)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Google Colab\n",
    "\n",
    "If you are running this notebook on Google Colab after the course ends:\n",
    "\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. Select a **GPU** accelerator:\n",
    "   - **T4** is available on the free tier and is sufficient for this notebook\n",
    "   - **A100** is available with Colab Pro and will be faster\n",
    "3. Click **Save**, then run all cells\n",
    "\n",
    "Embedding 12,622 short phrases takes approximately 2-5 minutes on a T4 GPU.\n",
    "Without a GPU, it will still work but may take 15-30 minutes on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Auto-Detection and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Environment Auto-Detection ---\ntry:\n    IS_COLAB = 'google.colab' in str(get_ipython())\nexcept NameError:\n    IS_COLAB = False\n\nif IS_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    PROJECT_ROOT = Path('/content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues')\nelse:\n    try:\n        PROJECT_ROOT = Path(__file__).resolve().parent.parent\n    except NameError:\n        PROJECT_ROOT = Path.cwd().parent\n\nDATA_DIR = PROJECT_ROOT / 'data'\nOUTPUT_DIR = PROJECT_ROOT / 'outputs'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Batch size for embedding generation.\n# Colab free-tier T4 GPUs have 16GB VRAM — use a smaller batch to avoid OOM.\n# Great Lakes V100/A40 and local GPUs with more VRAM can handle larger batches.\nBATCH_SIZE = 32 if IS_COLAB else 64\n\nprint(f'Project root: {PROJECT_ROOT}')\nprint(f'Data directory: {DATA_DIR}')\nprint(f'Batch size: {BATCH_SIZE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Unique Indicators\n\nThe input file `verified_indicators_unique.csv` is produced by `01_data_cleaning.ipynb`.\nIt contains one row per unique indicator string (12,622 indicators), with no wordplay labels.\n\nLabels are stored separately in `verified_clues_labeled.csv` and can be joined by indicator\nstring whenever needed for evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check that the input file exists before proceeding\ninput_file = DATA_DIR / 'verified_indicators_unique.csv'\nassert input_file.exists(), (\n    f'Missing input file: {input_file}\\n'\n    f'Run 01_data_cleaning.ipynb first to produce this file.'\n)\n\ndf_indicators = pd.read_csv(input_file)\nindicators_list = df_indicators['indicator'].tolist()\n\nprint(f'Loaded {len(indicators_list):,} unique indicators')\nprint(f'Examples: {indicators_list[:5]}')\nprint(f'Shortest: \"{min(indicators_list, key=len)}\" ({len(min(indicators_list, key=len))} chars)')\nprint(f'Longest: \"{max(indicators_list, key=len)}\" ({len(max(indicators_list, key=len))} chars)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BGE-M3 Embeddings\n",
    "\n",
    "We use the [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) model from the\n",
    "`sentence-transformers` library. BGE-M3 produces 1024-dimensional dense embeddings\n",
    "and is part of the CALE (Concept-Aligned Language Embeddings) family of models\n",
    "pretrained to distinguish word senses in context.\n",
    "\n",
    "**Why BGE-M3?** Our indicators are short phrases (1-6 words) that carry specific\n",
    "semantic meaning related to wordplay operations. BGE-M3 handles short text well\n",
    "and produces embeddings where semantically similar phrases (e.g., \"scrambled\" and\n",
    "\"mixed up\") are close in vector space. This is the settled model choice per\n",
    "FINDINGS_AND_DECISIONS.md.\n",
    "\n",
    "**What we are NOT doing:** We embed each indicator in isolation (not within its\n",
    "clue context). This is a settled decision — see FINDINGS_AND_DECISIONS.md for\n",
    "the rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BGE-M3 model\n",
    "# First run will download the model (~2.3 GB). Subsequent runs use the cached version.\n",
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "print(f'Model loaded: {model.get_sentence_embedding_dimension()} dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all unique indicators\n",
    "# show_progress_bar=True displays a tqdm progress bar during encoding\n",
    "embeddings = model.encode(\n",
    "    indicators_list,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f'Embeddings shape: {embeddings.shape}')\n",
    "print(f'Dtype: {embeddings.dtype}')\n",
    "print(f'Memory: {embeddings.nbytes / 1024**2:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs\n",
    "\n",
    "Two files are saved:\n",
    "\n",
    "1. **`embeddings_bge_m3_all.npy`** — NumPy array of shape (N, 1024) where N is the\n",
    "   number of unique indicators. Row `i` in this array corresponds to row `i` in the\n",
    "   indicator index CSV.\n",
    "\n",
    "2. **`indicator_index_all.csv`** — Maps each row number to its indicator string.\n",
    "   The CSV index (first column) is the row number in the embedding array. This is\n",
    "   the contract between the embedding file and the indicator identity.\n",
    "\n",
    "Downstream notebooks (Stage 3, 4, 5) should load these files rather than\n",
    "recomputing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedding matrix\n",
    "np.save(DATA_DIR / 'embeddings_bge_m3_all.npy', embeddings)\n",
    "print(f'Saved embeddings to {DATA_DIR / \"embeddings_bge_m3_all.npy\"}')\n",
    "\n",
    "# Save the indicator index (row number -> indicator string)\n",
    "df_indicators.to_csv(DATA_DIR / 'indicator_index_all.csv', index=True)\n",
    "print(f'Saved indicator index to {DATA_DIR / \"indicator_index_all.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Reload the saved files and verify that shapes match and the row mapping is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and verify\n",
    "embeddings_check = np.load(DATA_DIR / 'embeddings_bge_m3_all.npy')\n",
    "index_check = pd.read_csv(DATA_DIR / 'indicator_index_all.csv', index_col=0)\n",
    "\n",
    "assert embeddings_check.shape[0] == len(index_check), (\n",
    "    f'Shape mismatch: embeddings has {embeddings_check.shape[0]} rows, '\n",
    "    f'index has {len(index_check)} rows'\n",
    ")\n",
    "assert embeddings_check.shape[1] == 1024, (\n",
    "    f'Expected 1024 dimensions, got {embeddings_check.shape[1]}'\n",
    ")\n",
    "\n",
    "print(f'Embeddings: {embeddings_check.shape}')\n",
    "print(f'Index: {len(index_check)} rows')\n",
    "print(f'All checks passed.')\n",
    "\n",
    "# Spot-check: find a known indicator and verify it has a non-zero embedding\n",
    "spot_check = 'about'\n",
    "row = index_check[index_check['indicator'] == spot_check].index[0]\n",
    "norm = np.linalg.norm(embeddings_check[row])\n",
    "print(f'\\nSpot check: \"{spot_check}\" is at row {row}, embedding L2 norm = {norm:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}