{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOKvn6/O/2uQ0N8gawQ/c1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6L98M9FEF9P","executionInfo":{"status":"ok","timestamp":1770225588897,"user_tz":480,"elapsed":8533,"user":{"displayName":"Nathan Cantwell","userId":"15236319868573600434"}},"outputId":"43f9446f-e5f9-4a04-c3db-afbbd1bb46ff"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import string\n","import re\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')"]},{"cell_type":"code","source":["# Mount Google Drive (required every time)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"0CNEu_jrET3x","executionInfo":{"status":"error","timestamp":1770225710432,"user_tz":480,"elapsed":121521,"user":{"displayName":"Nathan Cantwell","userId":"15236319868573600434"}},"outputId":"af7afc21-e03f-4a81-fa4d-dcb95f0cd2af"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2973911950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive (required every time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}]},{"cell_type":"code","source":["# Define and check the paths\n","# PROJECT_ROOT assumes the shared Milestone II folder is in your root google drive\n","PROJECT_ROOT = '/content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues' # Nathan's Drive\n","DATA_DIR = f\"{PROJECT_ROOT}/data\"\n","NOTEBOOK_DIR = f\"{PROJECT_ROOT}/notebooks\"\n","\n","if not os.path.exists(PROJECT_ROOT):\n","    PROJECT_ROOT = os.path.abspath(\"..\")  # fallback for local runs"],"metadata":{"id":"zX_PZHgrEWBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read each CSV file into a DataFrame\n","df_clues = pd.read_csv(f'{DATA_DIR}/clues_raw.csv')\n","df_indicators = pd.read_csv(f'{DATA_DIR}/indicators_raw.csv')\n","df_ind_by_clue = pd.read_csv(f'{DATA_DIR}/indicators_by_clue_raw.csv')\n","df_ind_consolidated = pd.read_csv(f'{DATA_DIR}/indicators_consolidated_raw.csv')\n","df_charades = pd.read_csv(f'{DATA_DIR}/charades_raw.csv')\n","df_charades_by_clue = pd.read_csv(f'{DATA_DIR}/charades_by_clue_raw.csv')"],"metadata":{"id":"dG1En508EXZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instead of a string with redundant indices, extract only the clue_ids in\n","# brackets to create a list of integers\n","df_indicators[\"clue_ids\"] = (\n","    df_indicators[\"clue_ids\"]\n","    .str.findall(r\"\\[(\\d+)\\]\")\n","    .apply(lambda xs: [int(x) for x in xs])\n",")\n","\n","# Include a new column to keep track of how many clues have this indicator\n","df_indicators[\"num_clues\"] = df_indicators[\"clue_ids\"].apply(len)"],"metadata":{"id":"_xTV1V0VMjAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary where the key is the wordplay type, and the value is\n","# the list of associated unique indicators.\n","ind_by_wordplay_dict = {}\n","\n","for wordplay in df_ind_consolidated.columns:\n","  ind_by_wordplay_dict[wordplay] = df_ind_consolidated[wordplay].values[0].split('\\n')\n","\n","# See how many unique indicators there are for each type of wordplay\n","for wordplay in ind_by_wordplay_dict:\n","  print(f\"{wordplay}: {len(ind_by_wordplay_dict[wordplay])}\")"],"metadata":{"id":"pAVYHgxJM94W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators.shape, df_indicators.columns"],"metadata":{"id":"MUyk45XtK6Sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators.wordplay.value_counts().sort_values()"],"metadata":{"id":"_5QQgzbDLn2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators.indicator.value_counts().sort_values()\n","# Array of indicators that appear more than once in df, same as more than one wordplay type?\n","counts = df_indicators.groupby('indicator')['indicator'].transform('count')\n","df_indicators[counts > 1].indicator.unique()"],"metadata":{"id":"7TzggANEL1cY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators[df_indicators.indicator=='abnormal']"],"metadata":{"id":"_tRYgUHypSx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators[df_indicators.indicator=='caught']"],"metadata":{"id":"Jp1O6twrL8cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 888037 clues in the ind_by_clue df.\n","# 5730 of these have more than one indicator associated with them.\n","# 'to squeeze' seems like an indicator that is >1 word, but could be cleaned and retained.\n","df_ind_by_clue['ind_count'] = 8-df_ind_by_clue.isna().sum(axis=1)\n","print(df_ind_by_clue[df_ind_by_clue.ind_count>1].shape)\n","df_ind_by_clue.sort_values('ind_count', ascending=False)"],"metadata":{"id":"Z_rPkrtG07kP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_indicators.iloc[15732]"],"metadata":{"id":"cuB8nUry2KZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The indicator to extract could be simply \"reflected\".\n","df_clues[df_clues.clue_id==50741]['clue'].values"],"metadata":{"id":"e6idLWe0K-YQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 9097 rows with indicators >1 word count.\n","df_indicators['ind_wc'] = df_indicators['indicator'].apply(lambda x: len(str(x).split()))\n","df_long_ind = df_indicators[df_indicators['ind_wc'] > 1]\n","\n","# Try removing stopwords, maybe better to retain all, allow membership with multiple clusters\n","stop = stopwords.words('english')\n","df_long_ind['indicator_wo_stop'] = df_long_ind['indicator'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"],"metadata":{"id":"XRAzNZpj2la4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_long_ind"],"metadata":{"id":"G0kty4Sr3yVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_clues[df_clues.clue_id==412172]['clue'].values"],"metadata":{"id":"OrjgRoQC333e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_clues[df_clues.clue_id==412172]"],"metadata":{"id":"x15UMa3U4CUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind_by_wordplay_dict.keys()"],"metadata":{"id":"gB7wetOXMxuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Option 1: unique set of lemmatized indicator\n","\n","def ind_list(in_dict, key):\n","  # split words if indicator wc>1\n","  ser = pd.Series(in_dict[key]).apply(lambda x: str(x).split())\n","  # keep only unique indicators of this wordplay type.\n","  unique_inds = list(set(ser.sum()))\n","  lemmatizer = WordNetLemmatizer()\n","  lem_unique_inds = list(set([lemmatizer.lemmatize(word) for word in unique_inds]))\n","  lem_unique_inds.sort()\n","  return lem_unique_inds\n","\n","out = [ind_list(ind_by_wordplay_dict, key) for key in ind_by_wordplay_dict.keys()]\n","out = sum(out, [])\n","out = list(set(out))\n","out.sort()\n","print(len(out))\n","out"],"metadata":{"id":"CHVU3dgg5gcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Option 2: unique set of lemmatized indicator words,\n","# enhanced with POS tagging lemmatization.\n","\n","# Define a function to map Penn Treebank tags to WordNet tags\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN # Default to noun if no clear mapping\n","\n","\n","def ind_list(in_dict, key):\n","  # split words if indicator wc>1\n","  ser = pd.Series(in_dict[key]).apply(lambda x: str(x).split())\n","  # keep only unique indicators of this wordplay type.\n","  unique_inds = list(set(ser.sum()))\n","  # convert list of unique indicators to one string.\n","  unique_inds = ' '.join(unique_inds)\n","\n","  # Tokenize the text\n","  tokens = nltk.word_tokenize(unique_inds)\n","  # Perform POS tagging (NLTK uses Penn Treebank tags)\n","  pos_tags = nltk.pos_tag(tokens)\n","\n","  lemmatizer = WordNetLemmatizer()\n","  lem_unique_inds = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n","\n","  lem_unique_inds = list(set(lem_unique_inds))\n","  lem_unique_inds.sort()\n","\n","  return lem_unique_inds\n","\n","out = [ind_list(ind_by_wordplay_dict, key) for key in ind_by_wordplay_dict.keys()]\n","out = sum(out, [])\n","out = list(set(out))\n","out.sort()\n","print(len(out))\n","out"],"metadata":{"id":"iQ8uwtJD81JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Option 3: unique set of stemmed indicator words\n","\n","def ind_list(in_dict, key):\n","\n","  # split words if indicator wc>1\n","  ser = pd.Series(in_dict[key]).apply(lambda x: str(x).split())\n","  # keep only unique indicators of this wordplay type.\n","  unique_inds = list(set(ser.sum()))\n","  ps = PorterStemmer()\n","  stem_unique_inds = list(set([ps.stem(word) for word in unique_inds]))\n","  stem_unique_inds.sort()\n","  return stem_unique_inds\n","\n","out = [ind_list(ind_by_wordplay_dict, key) for key in ind_by_wordplay_dict.keys()]\n","out = sum(out, [])\n","out = list(set(out))\n","out.sort()\n","print(len(out))\n","out"],"metadata":{"id":"NykU2kXIBUtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find any punctuation in indicators.\n","pattern = f\"[{re.escape(string.punctuation)}]+\"\n","for key in ind_by_wordplay_dict.keys():\n","  words = ind_by_wordplay_dict[key]\n","  print(key, len(ind_by_wordplay_dict[key]), len([w for w in words if re.findall(pattern, w)]))"],"metadata":{"id":"RJJIm51DGypz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind_by_wordplay_dict['anagram']"],"metadata":{"id":"bGe-Ba4SLhxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = ind_by_wordplay_dict['anagram']\n","# Create a translation table that maps every punctuation character to None (removal)\n","translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n","# Apply the translation\n","new_words = [w.translate(translator) for w in words]\n","\n","new_words"],"metadata":{"id":"FXOxIdTzJEtS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Looked into the anagram indicators more... are these acting on the definition clue? Or is the anagram indicator containing the actual letters to use in the answer?\n","- Use a counter, giving more confidence in more frequent clue indicators?\n","- These '/' characters are strange, what is the actual text in the CCC that matches this indicator?"],"metadata":{"id":"2EKALD8XO0es"}}]}