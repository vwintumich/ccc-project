{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9444644",
   "metadata": {},
   "source": [
    "# Stage 5: Constrained and Targeted Experiments\n",
    "\n",
    "**Primary author:** Victoria\n",
    "**Builds on:** `04_clustering.ipynb` (Stage 4: unconstrained clustering results)\n",
    "**Prompt engineering:** Victoria\n",
    "**AI assistance:** Claude (Anthropic)\n",
    "**Environment:** Great Lakes (for definitions embedding); Colab (for subset experiments); Local (for label evaluation)\n",
    "\n",
    "**Research question: \"Does expert knowledge improve clustering, and do theoretically\n",
    "motivated subsets behave as predicted?\"**\n",
    "\n",
    "Stage 4 asked what structure emerges when we let clustering algorithms explore freely.\n",
    "The answer: strong local structure at fine granularity (282 HDBSCAN clusters, metrics\n",
    "improving up to k=250 for agglomerative), but no natural k=8 grouping and no stable\n",
    "intermediate level in HDBSCAN.\n",
    "\n",
    "This notebook introduces domain knowledge for the first time:\n",
    "- **Wordplay type labels** (Ho blog labels and algorithmically derived GT labels) to\n",
    "  evaluate whether unconstrained clusters correspond to known types\n",
    "- **Seed words** from expert sources to guide constrained clustering\n",
    "- **Subset experiments** to test specific hypotheses about which types should be easy\n",
    "  vs. hard to separate\n",
    "\n",
    "The four sections:\n",
    "1. **Setup, Load Data, Build Label Sets** — load all inputs and create per-indicator\n",
    "   label mappings\n",
    "2. **Label-Based Evaluation of NB 04 Results** — overlay labels on unconstrained\n",
    "   clusters to see what they captured\n",
    "3. **Constrained Agglomerative Clustering** — use seed words to guide clustering\n",
    "4. **Subset Experiments** — test separation and overlap hypotheses on targeted subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7167c1",
   "metadata": {},
   "source": [
    "## Running on Google Colab\n",
    "\n",
    "If running on Google Colab:\n",
    "\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. A GPU is not required for Sections 1-2 (label evaluation). GPU is needed only for\n",
    "   definitions embedding in Section 3.\n",
    "3. Click **Save**, then run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36da475",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40699a21",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58646e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48ac5",
   "metadata": {},
   "source": [
    "### Environment Auto-Detection and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d899c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/victoria/Desktop/MADS/ccc-project/indicator_clustering\n",
      "Data directory: /Users/victoria/Desktop/MADS/ccc-project/indicator_clustering/data\n",
      "Output directory: /Users/victoria/Desktop/MADS/ccc-project/indicator_clustering/outputs\n",
      "Figures directory: /Users/victoria/Desktop/MADS/ccc-project/indicator_clustering/outputs/figures\n"
     ]
    }
   ],
   "source": [
    "# --- Environment Auto-Detection ---\n",
    "try:\n",
    "    IS_COLAB = 'google.colab' in str(get_ipython())\n",
    "except NameError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "IS_GREATLAKES = 'SLURM_JOB_ID' in os.environ\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues')\n",
    "elif IS_GREATLAKES:\n",
    "    # Update YOUR_UNIQNAME to your actual UMich uniqname\n",
    "    PROJECT_ROOT = Path('/scratch/YOUR_UNIQNAME/ccc_project')\n",
    "else:\n",
    "    try:\n",
    "        PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Data directory: {DATA_DIR}')\n",
    "print(f'Output directory: {OUTPUT_DIR}')\n",
    "print(f'Figures directory: {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beda800",
   "metadata": {},
   "source": [
    "### Input File Validation\n",
    "\n",
    "This notebook requires outputs from Stages 1–4. We check that all required files exist\n",
    "before proceeding, rather than failing partway through.\n",
    "\n",
    "| File | Produced by | Description |\n",
    "|------|-------------|-------------|\n",
    "| `embeddings_umap_10d.npy` | Stage 3 | 10D UMAP embeddings for clustering |\n",
    "| `embeddings_umap_2d.npy` | Stage 3 | 2D UMAP embeddings for visualization |\n",
    "| `indicator_index_all.csv` | Stage 2 | Row-to-indicator-string mapping |\n",
    "| `verified_clues_labeled.csv` | Stage 1 | Clue-indicator pairs with Ho and GT labels |\n",
    "| `cluster_labels_hdbscan_eps_0p0000.csv` | Stage 4 | Best HDBSCAN labels (eps=0.0) |\n",
    "| `cluster_labels_agglo_k8.csv` | Stage 4 | Agglomerative k=8 labels |\n",
    "| `cluster_labels_agglo_k10.csv` | Stage 4 | Agglomerative k=10 labels |\n",
    "| `cluster_labels_agglo_k34.csv` | Stage 4 | Agglomerative k=34 labels |\n",
    "| `clustering_metrics_summary.csv` | Stage 4 | Metrics from all Stage 4 runs |\n",
    "| `wordplay_seeds.xlsx` | Manual (expert) | Seed words for constrained clustering (Section 3) |\n",
    "\n",
    "**New in this notebook:** `verified_clues_labeled.csv` and `wordplay_seeds.xlsx`. These\n",
    "were deliberately excluded from Notebook 04 to keep the unconstrained analysis label-free.\n",
    "This is where domain knowledge enters the clustering pipeline for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1136f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All input files found.\n"
     ]
    }
   ],
   "source": [
    "required_files = {\n",
    "    # Stage 2-3: embeddings\n",
    "    'embeddings_umap_10d.npy': 'Run 03_dimensionality_reduction.ipynb',\n",
    "    'embeddings_umap_2d.npy': 'Run 03_dimensionality_reduction.ipynb',\n",
    "    'indicator_index_all.csv': 'Run 02_embedding_generation.ipynb',\n",
    "    # Stage 1: labels\n",
    "    'verified_clues_labeled.csv': 'Run 01_data_cleaning.ipynb',\n",
    "    # Stage 4: cluster assignments\n",
    "    'cluster_labels_hdbscan_eps_0p0000.csv': 'Run 04_clustering.ipynb',\n",
    "    'cluster_labels_agglo_k8.csv': 'Run 04_clustering.ipynb',\n",
    "    'cluster_labels_agglo_k10.csv': 'Run 04_clustering.ipynb',\n",
    "    'cluster_labels_agglo_k34.csv': 'Run 04_clustering.ipynb',\n",
    "    # Seed words (used in Section 3)\n",
    "    'wordplay_seeds.xlsx': 'Manually created from expert sources (Minute Cryptic, CC for Dummies)',\n",
    "}\n",
    "\n",
    "# Metrics summary is in OUTPUT_DIR, not DATA_DIR\n",
    "required_output_files = {\n",
    "    'clustering_metrics_summary.csv': 'Run 04_clustering.ipynb',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for fname, fix_msg in required_files.items():\n",
    "    fpath = DATA_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f'MISSING: {fpath}')\n",
    "        print(f'  Fix: {fix_msg}')\n",
    "        all_ok = False\n",
    "\n",
    "for fname, fix_msg in required_output_files.items():\n",
    "    fpath = OUTPUT_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f'MISSING: {fpath}')\n",
    "        print(f'  Fix: {fix_msg}')\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print('All input files found.')\n",
    "else:\n",
    "    raise FileNotFoundError('One or more required files are missing. See messages above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bf697",
   "metadata": {},
   "source": [
    "### Load Embeddings and Indicator Index\n",
    "\n",
    "These are the same files loaded in Notebook 04. The 10D embeddings are used for\n",
    "clustering; the 2D embeddings are used for scatter plot visualization. The indicator\n",
    "index maps each row number to its indicator string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa024c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10D embeddings shape: (12622, 10)\n",
      "2D embeddings shape:  (12622, 2)\n",
      "Indicator count:      12,622\n",
      "Shape checks passed.\n"
     ]
    }
   ],
   "source": [
    "# Load UMAP embeddings\n",
    "embeddings_10d = np.load(DATA_DIR / 'embeddings_umap_10d.npy')\n",
    "embeddings_2d = np.load(DATA_DIR / 'embeddings_umap_2d.npy')\n",
    "\n",
    "# Load indicator index (maps row i -> indicator string)\n",
    "df_index = pd.read_csv(DATA_DIR / 'indicator_index_all.csv', index_col=0)\n",
    "indicator_names = df_index['indicator'].values\n",
    "\n",
    "n_indicators = len(df_index)\n",
    "print(f'10D embeddings shape: {embeddings_10d.shape}')\n",
    "print(f'2D embeddings shape:  {embeddings_2d.shape}')\n",
    "print(f'Indicator count:      {n_indicators:,}')\n",
    "\n",
    "assert embeddings_10d.shape == (n_indicators, 10)\n",
    "assert embeddings_2d.shape == (n_indicators, 2)\n",
    "print('Shape checks passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7bf477",
   "metadata": {},
   "source": [
    "### Load Wordplay Labels\n",
    "\n",
    "**This is the first time labels enter the clustering pipeline.**\n",
    "\n",
    "`verified_clues_labeled.csv` contains one row per verified (clue_id, indicator) pair —\n",
    "76,015 rows total. Each row carries two kinds of wordplay labels:\n",
    "\n",
    "#### Ho Labels (`wordplay_ho`)\n",
    "\n",
    "These are the original wordplay type labels parsed from cryptic crossword blog\n",
    "commentary by George Ho. They cover **all 8 wordplay types**: anagram, reversal,\n",
    "hidden, container, insertion, deletion, homophone, and alternation. Every row in the\n",
    "file has a Ho label.\n",
    "\n",
    "**Strengths:** Complete coverage of all types; large sample size.\n",
    "**Weaknesses:** Parsed from informal blog text, so some labels may be noisy or\n",
    "inconsistent. The parsing treats the blogger's description as ground truth, but\n",
    "bloggers occasionally mislabel or use ambiguous terminology.\n",
    "\n",
    "#### GT Labels (`wordplay_gt`)\n",
    "\n",
    "These are algorithmically derived \"ground truth\" labels produced by Victoria's\n",
    "verification code in Stage 1. The algorithm checks whether the answer can be\n",
    "mechanically derived from the clue components using the rules of each wordplay type.\n",
    "It covers **only 4 types**: hidden, reversal, alternation, and anagram — the types\n",
    "where the transformation can be verified algorithmically (e.g., checking that the\n",
    "answer's letters appear consecutively in the fodder for hidden-word clues).\n",
    "\n",
    "**Strengths:** High precision — if the algorithm says it's a hidden-word indicator,\n",
    "the mechanical check confirms it. No human judgment involved.\n",
    "**Weaknesses:** Only 4 of 8 types are covered. Container, insertion, deletion, and\n",
    "homophone require contextual understanding that the algorithm cannot perform. About\n",
    "74% of rows have no GT label.\n",
    "\n",
    "#### Why Both Matter\n",
    "\n",
    "Neither label set supersedes the other:\n",
    "- **Ho labels** give us full coverage but lower precision\n",
    "- **GT labels** give us high precision but partial coverage\n",
    "\n",
    "When they disagree, that's informative — it highlights indicators where the blog\n",
    "commentary and the mechanical verification diverge. The `label_match` column tracks\n",
    "this agreement (92.6% match rate where GT exists).\n",
    "\n",
    "#### Multi-Label Indicators\n",
    "\n",
    "A single indicator string can appear under **multiple wordplay types**. For example,\n",
    "\"about\" is used as a container indicator, a reversal indicator, and an anagram\n",
    "indicator in different clues. This is linguistically real — the word genuinely serves\n",
    "multiple functions — not parsing noise. In `verified_clues_labeled.csv`, such an\n",
    "indicator appears in multiple rows with different `wordplay_ho` values.\n",
    "\n",
    "For this analysis, we build:\n",
    "- A **label set** per indicator: all Ho (or GT) types it appears under (for overlay\n",
    "  plots where a multi-label indicator should appear in every relevant type's subplot)\n",
    "- A **primary label**: the single most frequent Ho (or GT) type across that\n",
    "  indicator's clue appearances (for heatmaps and coloring where a single assignment\n",
    "  is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb318c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels file: 76,015 rows\n",
      "Unique indicators: 12,622\n",
      "\n",
      "Ho label distribution (instance-level):\n",
      "wordplay_ho\n",
      "anagram        38226\n",
      "container      10836\n",
      "reversal       10149\n",
      "insertion       8305\n",
      "homophone       3642\n",
      "hidden          2595\n",
      "deletion        1608\n",
      "alternation      654\n",
      "\n",
      "GT label distribution (NaN = no GT label):\n",
      "wordplay_gt\n",
      "NaN            56348\n",
      "anagram        15346\n",
      "hidden          2556\n",
      "reversal        1506\n",
      "alternation      259\n"
     ]
    }
   ],
   "source": [
    "# Load the full labels file\n",
    "df_labels = pd.read_csv(DATA_DIR / 'verified_clues_labeled.csv')\n",
    "\n",
    "print(f'Labels file: {len(df_labels):,} rows')\n",
    "print(f'Unique indicators: {df_labels[\"indicator\"].nunique():,}')\n",
    "print(f'\\nHo label distribution (instance-level):')\n",
    "print(df_labels['wordplay_ho'].value_counts().to_string())\n",
    "print(f'\\nGT label distribution (NaN = no GT label):')\n",
    "print(df_labels['wordplay_gt'].value_counts(dropna=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c5f36",
   "metadata": {},
   "source": [
    "### Build Per-Indicator Label Sets\n",
    "\n",
    "For each unique indicator, we compute:\n",
    "\n",
    "1. **`ho_labels`** — the set of all Ho wordplay types this indicator appears under\n",
    "   (e.g., `{'container', 'reversal', 'anagram'}` for \"about\")\n",
    "2. **`gt_labels`** — the set of all GT wordplay types (may be empty if no GT label\n",
    "   exists)\n",
    "3. **`primary_ho`** — the single Ho type this indicator is most frequently labeled as\n",
    "   (the mode across all its clue appearances)\n",
    "4. **`primary_gt`** — the single most frequent GT type, or NaN if no GT labels exist\n",
    "\n",
    "The label sets (1–2) are used for the per-type overlay plots, where an indicator should\n",
    "appear in every subplot for every type it belongs to. The primary labels (3–4) are used\n",
    "for heatmaps and single-color scatter plots where each indicator needs exactly one label.\n",
    "\n",
    "We align everything with `indicator_index_all.csv` so that row `i` in the label arrays\n",
    "corresponds to row `i` in the embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e32e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataframe: 12,622 indicators\n",
      "Indicators with GT labels: 5,827 (46.2%)\n",
      "Indicators without GT labels: 6,795\n"
     ]
    }
   ],
   "source": [
    "# --- Build label sets per indicator ---\n",
    "\n",
    "# Ho label sets: for each indicator, the set of all Ho types it appears under\n",
    "ho_label_sets = (\n",
    "    df_labels\n",
    "    .groupby('indicator')['wordplay_ho']\n",
    "    .apply(lambda x: frozenset(x.dropna().unique()))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# GT label sets: for each indicator, the set of all GT types it appears under\n",
    "gt_label_sets = (\n",
    "    df_labels\n",
    "    .groupby('indicator')['wordplay_gt']\n",
    "    .apply(lambda x: frozenset(x.dropna().unique()))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Primary Ho label: the most common Ho label across all clue appearances\n",
    "primary_ho_map = (\n",
    "    df_labels\n",
    "    .groupby('indicator')['wordplay_ho']\n",
    "    .agg(lambda x: x.value_counts().index[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Primary GT label: the most common GT label, if any GT labels exist for this indicator\n",
    "df_labels_gt = df_labels[df_labels['wordplay_gt'].notna()]\n",
    "primary_gt_map = (\n",
    "    df_labels_gt\n",
    "    .groupby('indicator')['wordplay_gt']\n",
    "    .agg(lambda x: x.value_counts().index[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# --- Create master dataframe aligned with embedding order ---\n",
    "# This ensures row i matches embeddings_10d[i] and embeddings_2d[i]\n",
    "df_master = df_index[['indicator']].copy()\n",
    "df_master['ho_labels'] = df_master['indicator'].map(ho_label_sets)\n",
    "df_master['gt_labels'] = df_master['indicator'].map(gt_label_sets)\n",
    "df_master['primary_ho'] = df_master['indicator'].map(primary_ho_map)\n",
    "df_master['primary_gt'] = df_master['indicator'].map(primary_gt_map)\n",
    "\n",
    "# Verify alignment: every indicator should have at least one Ho label\n",
    "assert df_master['primary_ho'].notna().all(), 'Some indicators have no Ho label!'\n",
    "print(f'Master dataframe: {len(df_master):,} indicators')\n",
    "print(f'Indicators with GT labels: {df_master[\"primary_gt\"].notna().sum():,} '\n",
    "      f'({df_master[\"primary_gt\"].notna().mean():.1%})')\n",
    "print(f'Indicators without GT labels: {df_master[\"primary_gt\"].isna().sum():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5993e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Primary Ho Label Distribution (unique indicators) ===\n",
      "       anagram: 6,453 (51.1%)\n",
      "     container: 1,523 (12.1%)\n",
      "     insertion: 1,385 (11.0%)\n",
      "      reversal: 1,350 (10.7%)\n",
      "      deletion:   604 (4.8%)\n",
      "        hidden:   560 (4.4%)\n",
      "     homophone:   541 (4.3%)\n",
      "   alternation:   206 (1.6%)\n",
      "\n",
      "=== Primary GT Label Distribution (unique indicators) ===\n",
      "       anagram: 4,364 (34.6%)\n",
      "        hidden:   867 (6.9%)\n",
      "      reversal:   474 (3.8%)\n",
      "   alternation:   122 (1.0%)\n",
      "       (no GT): 6,795 (53.8%)\n",
      "\n",
      "Multi-label indicators (Ho): 1,248 (9.9%)\n",
      "Multi-label indicators (GT): 304\n",
      "\n",
      "Examples of multi-label indicators (Ho):\n",
      "  \"a bit of\" → hidden, insertion\n",
      "  \"a little\" → hidden, insertion\n",
      "  \"abandoned\" → anagram, deletion\n",
      "  \"abducted by\" → hidden, insertion\n",
      "  \"aboard\" → container, hidden, insertion\n",
      "  \"aborted\" → anagram, deletion\n",
      "  \"about\" → anagram, container, hidden, insertion, reversal\n",
      "  \"absorbed\" → container, hidden\n"
     ]
    }
   ],
   "source": [
    "# --- Label statistics ---\n",
    "\n",
    "# Ho type counts (unique indicators, using primary label)\n",
    "print('=== Primary Ho Label Distribution (unique indicators) ===')\n",
    "ho_counts = df_master['primary_ho'].value_counts()\n",
    "for wtype, count in ho_counts.items():\n",
    "    print(f'  {wtype:>12s}: {count:>5,} ({count / len(df_master):.1%})')\n",
    "\n",
    "print(f'\\n=== Primary GT Label Distribution (unique indicators) ===')\n",
    "gt_counts = df_master['primary_gt'].value_counts()\n",
    "for wtype, count in gt_counts.items():\n",
    "    print(f'  {wtype:>12s}: {count:>5,} ({count / len(df_master):.1%})')\n",
    "n_no_gt = df_master['primary_gt'].isna().sum()\n",
    "print(f'  {\"(no GT)\":>12s}: {n_no_gt:>5,} ({n_no_gt / len(df_master):.1%})')\n",
    "\n",
    "# Multi-label indicators\n",
    "n_multi_ho = sum(1 for s in df_master['ho_labels'] if len(s) > 1)\n",
    "print(f'\\nMulti-label indicators (Ho): {n_multi_ho:,} '\n",
    "      f'({n_multi_ho / len(df_master):.1%})')\n",
    "\n",
    "n_multi_gt = sum(1 for s in df_master['gt_labels'] if len(s) > 1)\n",
    "print(f'Multi-label indicators (GT): {n_multi_gt:,}')\n",
    "\n",
    "# Examples of multi-label indicators\n",
    "multi_ho_df = df_master[df_master['ho_labels'].apply(len) > 1]\n",
    "print(f'\\nExamples of multi-label indicators (Ho):')\n",
    "for _, row in multi_ho_df.head(8).iterrows():\n",
    "    types_str = ', '.join(sorted(row['ho_labels']))\n",
    "    print(f'  \"{row[\"indicator\"]}\" \\u2192 {types_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5db8b",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Label-Based Evaluation of Notebook 04 Results\n",
    "\n",
    "Now that we have wordplay type labels attached to each indicator, we can answer:\n",
    "**do the unconstrained clusters from Notebook 04 correspond to known wordplay types?**\n",
    "\n",
    "Recall from NB 04:\n",
    "- **HDBSCAN at eps=0.0** found 282 tight clusters with 33.4% noise — high silhouette\n",
    "  (0.631) but many excluded points\n",
    "- **Agglomerative k=8** is the reference point matching the number of Ho types\n",
    "- **Agglomerative k=10** is the local silhouette optimum (the only evidence for coarse\n",
    "  structure in the metrics sweep)\n",
    "- **Agglomerative k=34** is a mid-range granularity where clusters become semantically\n",
    "  more coherent\n",
    "\n",
    "This section produces:\n",
    "1. **Per-type overlay plots** — where does each wordplay type's indicators live in the\n",
    "   2D UMAP space? This shows the \"ground truth\" spatial layout of types.\n",
    "2. **Per-cluster type distribution heatmaps** — for each clustering run, what is the\n",
    "   Ho type composition of each cluster? This reveals whether clusters are type-pure or\n",
    "   mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63469d73",
   "metadata": {},
   "source": [
    "### Load Cluster Labels from Notebook 04\n",
    "\n",
    "We load the cluster assignments saved by NB 04 for the four runs we want to evaluate.\n",
    "Each CSV has columns `indicator` and `cluster_label`, aligned with\n",
    "`indicator_index_all.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fa8906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN eps=0.0: 282 clusters, 4212 noise points\n",
      "Agglomerative k=8: 8 clusters, 0 noise points\n",
      "Agglomerative k=10: 10 clusters, 0 noise points\n",
      "Agglomerative k=34: 34 clusters, 0 noise points\n",
      "\n",
      "Loaded 4 clustering runs.\n"
     ]
    }
   ],
   "source": [
    "# Define the clustering runs to evaluate\n",
    "runs_to_evaluate = {\n",
    "    'HDBSCAN eps=0.0': {\n",
    "        'file': 'cluster_labels_hdbscan_eps_0p0000.csv',\n",
    "        'has_noise': True,\n",
    "    },\n",
    "    'Agglomerative k=8': {\n",
    "        'file': 'cluster_labels_agglo_k8.csv',\n",
    "        'has_noise': False,\n",
    "    },\n",
    "    'Agglomerative k=10': {\n",
    "        'file': 'cluster_labels_agglo_k10.csv',\n",
    "        'has_noise': False,\n",
    "    },\n",
    "    'Agglomerative k=34': {\n",
    "        'file': 'cluster_labels_agglo_k34.csv',\n",
    "        'has_noise': False,\n",
    "    },\n",
    "}\n",
    "\n",
    "cluster_labels_dict = {}\n",
    "for run_name, run_info in runs_to_evaluate.items():\n",
    "    df_cl = pd.read_csv(DATA_DIR / run_info['file'])\n",
    "    # Verify alignment with indicator index\n",
    "    assert list(df_cl['indicator']) == list(indicator_names), (\n",
    "        f'Indicator order mismatch in {run_info[\"file\"]}'\n",
    "    )\n",
    "    cluster_labels_dict[run_name] = df_cl['cluster_label'].values\n",
    "    n_clusters = len(set(df_cl['cluster_label'])) - (1 if run_info['has_noise'] else 0)\n",
    "    n_noise = (df_cl['cluster_label'] == -1).sum() if run_info['has_noise'] else 0\n",
    "    print(f'{run_name}: {n_clusters} clusters, {n_noise} noise points')\n",
    "\n",
    "print(f'\\nLoaded {len(cluster_labels_dict)} clustering runs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba4e90",
   "metadata": {},
   "source": [
    "### Color Palette and Helper Functions\n",
    "\n",
    "We define a consistent color palette for the 8 Ho wordplay types. The same colors are\n",
    "used in every plot throughout this notebook and in Notebook 06, making it easy to track\n",
    "types across visualizations.\n",
    "\n",
    "The palette is chosen to be colorblind-accessible where possible, with high-frequency\n",
    "types (anagram, container, reversal) getting the most visually distinct colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867916ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ho type indicator counts (an indicator can appear in multiple types):\n",
      "       anagram: 6,610\n",
      "      reversal: 1,495\n",
      "        hidden:   971\n",
      "     container: 1,728\n",
      "     insertion: 1,915\n",
      "      deletion:   695\n",
      "     homophone:   565\n",
      "   alternation:   216\n",
      "\n",
      "GT type indicator counts:\n",
      "       anagram: 4,436\n",
      "      reversal:   631\n",
      "        hidden:   964\n",
      "   alternation:   131\n"
     ]
    }
   ],
   "source": [
    "# --- Consistent color palette for wordplay types ---\n",
    "# Used in all overlay plots, heatmaps, and scatter plots throughout NB 05 and 06\n",
    "\n",
    "HO_TYPES = ['anagram', 'reversal', 'hidden', 'container',\n",
    "            'insertion', 'deletion', 'homophone', 'alternation']\n",
    "\n",
    "GT_TYPES = ['anagram', 'reversal', 'hidden', 'alternation']\n",
    "\n",
    "TYPE_COLORS = {\n",
    "    'anagram':     '#e41a1c',  # red\n",
    "    'reversal':    '#377eb8',  # blue\n",
    "    'hidden':      '#4daf4a',  # green\n",
    "    'container':   '#984ea3',  # purple\n",
    "    'insertion':   '#ff7f00',  # orange\n",
    "    'deletion':    '#a65628',  # brown\n",
    "    'homophone':   '#f781bf',  # pink\n",
    "    'alternation': '#999999',  # gray\n",
    "}\n",
    "\n",
    "# Pre-compute boolean masks for each Ho type:\n",
    "# ho_type_masks['anagram'][i] is True if indicator i has ever been labeled 'anagram'\n",
    "ho_type_masks = {}\n",
    "for wtype in HO_TYPES:\n",
    "    ho_type_masks[wtype] = np.array([\n",
    "        wtype in label_set for label_set in df_master['ho_labels']\n",
    "    ])\n",
    "\n",
    "# Same for GT types\n",
    "gt_type_masks = {}\n",
    "for wtype in GT_TYPES:\n",
    "    gt_type_masks[wtype] = np.array([\n",
    "        wtype in label_set for label_set in df_master['gt_labels']\n",
    "    ])\n",
    "\n",
    "# Print type counts for overlay reference\n",
    "print('Ho type indicator counts (an indicator can appear in multiple types):')\n",
    "for wtype in HO_TYPES:\n",
    "    n = ho_type_masks[wtype].sum()\n",
    "    print(f'  {wtype:>12s}: {n:>5,}')\n",
    "\n",
    "print(f'\\nGT type indicator counts:')\n",
    "for wtype in GT_TYPES:\n",
    "    n = gt_type_masks[wtype].sum()\n",
    "    print(f'  {wtype:>12s}: {n:>5,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2257a",
   "metadata": {},
   "source": [
    "### Per-Type Overlay: Ho Labels\n",
    "\n",
    "Each subplot below highlights the indicators belonging to one Ho wordplay type,\n",
    "plotted on top of the full 2D UMAP cloud (shown in light gray). This reveals\n",
    "**where each type lives in the embedding space**:\n",
    "\n",
    "- **Concentrated clusters:** If a type's indicators form a tight region, the\n",
    "  embedding model captured something distinctive about that type's vocabulary.\n",
    "- **Dispersed clouds:** If a type's indicators are scattered across the full space,\n",
    "  the type's vocabulary is semantically diverse and may not form a natural cluster.\n",
    "- **Overlapping types:** If two types occupy the same region, their indicators share\n",
    "  semantic properties and will be hard to separate by clustering.\n",
    "\n",
    "These plots use all Ho labels (not just the primary label), so a multi-label indicator\n",
    "like \"about\" appears in every type subplot where it has been used. This gives the\n",
    "complete picture of where each type's vocabulary lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b60208",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(22, 10))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, wtype in enumerate(HO_TYPES):\n",
    "    ax = axes_flat[i]\n",
    "    mask = ho_type_masks[wtype]\n",
    "    n_type = mask.sum()\n",
    "\n",
    "    # Background: all indicators in light gray\n",
    "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "               s=1, alpha=0.08, color='lightgray', rasterized=True)\n",
    "\n",
    "    # Foreground: this type's indicators\n",
    "    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "               s=3, alpha=0.4, color=TYPE_COLORS[wtype], rasterized=True)\n",
    "\n",
    "    ax.set_title(f'{wtype} (n={n_type:,})', fontsize=11,\n",
    "                 color=TYPE_COLORS[wtype], fontweight='bold')\n",
    "    ax.set_xlabel('UMAP 1', fontsize=8)\n",
    "    ax.set_ylabel('UMAP 2', fontsize=8)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "plt.suptitle('Ho Label Overlay: Where Each Wordplay Type Lives in Embedding Space',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'overlay_ho_types.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"overlay_ho_types.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adf7fc",
   "metadata": {},
   "source": [
    "### Per-Type Overlay: GT Labels\n",
    "\n",
    "The same visualization using the algorithmically derived GT labels. Only 4 types are\n",
    "covered (anagram, reversal, hidden, alternation). Indicators without a GT label are\n",
    "part of the gray background only.\n",
    "\n",
    "Comparing this to the Ho overlay above reveals:\n",
    "- GT labels tend to highlight a **subset** of each type's indicators (the ones where\n",
    "  the transformation could be mechanically verified)\n",
    "- If the GT-highlighted region is a spatial subset of the Ho-highlighted region for\n",
    "  the same type, the two label sources are consistent\n",
    "- If GT highlights indicators in a different part of the space than Ho, there may be\n",
    "  labeling disagreements worth investigating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "\n",
    "for i, wtype in enumerate(GT_TYPES):\n",
    "    ax = axes[i]\n",
    "    mask = gt_type_masks[wtype]\n",
    "    n_type = mask.sum()\n",
    "\n",
    "    # Background: all indicators in light gray\n",
    "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "               s=1, alpha=0.08, color='lightgray', rasterized=True)\n",
    "\n",
    "    # Foreground: this type's GT-verified indicators\n",
    "    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "               s=4, alpha=0.5, color=TYPE_COLORS[wtype], rasterized=True)\n",
    "\n",
    "    ax.set_title(f'{wtype} — GT verified (n={n_type:,})', fontsize=11,\n",
    "                 color=TYPE_COLORS[wtype], fontweight='bold')\n",
    "    ax.set_xlabel('UMAP 1', fontsize=8)\n",
    "    ax.set_ylabel('UMAP 2', fontsize=8)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "plt.suptitle('GT Label Overlay: Algorithmically Verified Types in Embedding Space',\n",
    "             fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'overlay_gt_types.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"overlay_gt_types.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3af50",
   "metadata": {},
   "source": [
    "### Per-Cluster Type Distribution Heatmaps\n",
    "\n",
    "For each of the four clustering runs from NB 04, we create a heatmap showing the Ho\n",
    "type composition of each cluster. Each row is a cluster, each column is a wordplay\n",
    "type, and each cell shows the **proportion** of that cluster's indicators that have\n",
    "that type as their primary Ho label. Rows sum to 1.0.\n",
    "\n",
    "**How to read these heatmaps:**\n",
    "- A **pure cluster** has one bright cell in its row and near-zero everywhere else —\n",
    "  the cluster captured a single wordplay type.\n",
    "- A **mixed cluster** has color spread across multiple columns — it contains a blend\n",
    "  of types that the algorithm could not separate.\n",
    "- The **dominant type** of each cluster is the column with the highest value.\n",
    "- Clusters are sorted by size (largest at top) for readability.\n",
    "\n",
    "For HDBSCAN, which labels some points as noise (cluster = −1), we include a separate\n",
    "\"noise\" row showing the type distribution of noise points. If noise points are\n",
    "distributed similarly to the full dataset, the density-based method is not\n",
    "systematically excluding any particular type. If noise is concentrated in certain\n",
    "types, those types' indicators may be more dispersed in the embedding space.\n",
    "\n",
    "These heatmaps use the **primary Ho label** (single most common type per indicator)\n",
    "to avoid double-counting multi-label indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_type_distribution(cluster_labels, primary_ho, ho_types, type_colors,\n",
    "                           title, filename, max_clusters=20, show_noise=False):\n",
    "    \"\"\"Heatmap showing the Ho type composition of each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_labels : array of int\n",
    "        Cluster assignment for each indicator (-1 = noise for HDBSCAN).\n",
    "    primary_ho : array of str\n",
    "        Primary Ho label for each indicator.\n",
    "    ho_types : list of str\n",
    "        Ordered list of Ho type names (column order in heatmap).\n",
    "    type_colors : dict\n",
    "        Colors per type (used for column header coloring).\n",
    "    title : str\n",
    "        Plot title.\n",
    "    filename : str\n",
    "        Output filename (saved to FIGURES_DIR).\n",
    "    max_clusters : int\n",
    "        Maximum number of cluster rows to display (largest first).\n",
    "    show_noise : bool\n",
    "        If True, include a 'noise' row for cluster=-1 points.\n",
    "    \"\"\"\n",
    "    df_temp = pd.DataFrame({\n",
    "        'cluster': cluster_labels,\n",
    "        'type': primary_ho,\n",
    "    })\n",
    "\n",
    "    # Separate noise if applicable\n",
    "    if show_noise:\n",
    "        noise_df = df_temp[df_temp['cluster'] == -1]\n",
    "        df_clean = df_temp[df_temp['cluster'] != -1]\n",
    "    else:\n",
    "        noise_df = pd.DataFrame()\n",
    "        df_clean = df_temp\n",
    "\n",
    "    # Crosstab: rows = cluster, columns = type\n",
    "    ct = pd.crosstab(df_clean['cluster'], df_clean['type'])\n",
    "    for t in ho_types:\n",
    "        if t not in ct.columns:\n",
    "            ct[t] = 0\n",
    "    ct = ct[ho_types]\n",
    "\n",
    "    # Sort by cluster size (descending)\n",
    "    ct = ct.loc[ct.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "    # Limit to top N clusters if there are too many rows\n",
    "    if len(ct) > max_clusters:\n",
    "        ct = ct.head(max_clusters)\n",
    "\n",
    "    # Add noise row if applicable\n",
    "    if show_noise and len(noise_df) > 0:\n",
    "        noise_counts = noise_df['type'].value_counts()\n",
    "        noise_row = pd.DataFrame(\n",
    "            [[noise_counts.get(t, 0) for t in ho_types]],\n",
    "            columns=ho_types,\n",
    "            index=['noise']\n",
    "        )\n",
    "        ct = pd.concat([ct, noise_row])\n",
    "\n",
    "    # Row labels with cluster sizes\n",
    "    sizes = ct.sum(axis=1).astype(int)\n",
    "    row_labels = [f'{idx} (n={int(sizes[idx]):,})' for idx in ct.index]\n",
    "\n",
    "    # Normalize by row (proportions)\n",
    "    ct_norm = ct.div(ct.sum(axis=1), axis=0)\n",
    "\n",
    "    # Decide whether to annotate cells (skip for large heatmaps)\n",
    "    n_rows = len(ct_norm)\n",
    "    do_annot = n_rows <= 15\n",
    "    height = max(4, n_rows * 0.45 + 2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, height))\n",
    "    sns.heatmap(\n",
    "        ct_norm.values.astype(float),\n",
    "        annot=ct_norm.values.astype(float) if do_annot else False,\n",
    "        fmt='.2f' if do_annot else '',\n",
    "        cmap='YlOrRd',\n",
    "        ax=ax,\n",
    "        vmin=0, vmax=0.8,\n",
    "        linewidths=0.5,\n",
    "        xticklabels=ho_types,\n",
    "        yticklabels=row_labels,\n",
    "    )\n",
    "    ax.set_title(title, fontsize=13, pad=12)\n",
    "    ax.set_xlabel('Primary Ho Wordplay Type', fontsize=11)\n",
    "    ax.set_ylabel('Cluster (sorted by size)', fontsize=11)\n",
    "\n",
    "    # Color the x-axis tick labels by type for visual consistency\n",
    "    for tick_label in ax.get_xticklabels():\n",
    "        wtype = tick_label.get_text()\n",
    "        if wtype in type_colors:\n",
    "            tick_label.set_color(type_colors[wtype])\n",
    "            tick_label.set_fontweight('bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Saved: {FIGURES_DIR / filename}')\n",
    "\n",
    "    return ct_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate heatmaps for each clustering run ---\n",
    "primary_ho_array = df_master['primary_ho'].values\n",
    "\n",
    "type_dist_results = {}\n",
    "for run_name, labels in cluster_labels_dict.items():\n",
    "    run_info = runs_to_evaluate[run_name]\n",
    "    # Create a filename-safe version of the run name\n",
    "    safe_name = run_name.lower().replace(' ', '_').replace('=', '').replace('.', '')\n",
    "    ct_norm = plot_type_distribution(\n",
    "        cluster_labels=labels,\n",
    "        primary_ho=primary_ho_array,\n",
    "        ho_types=HO_TYPES,\n",
    "        type_colors=TYPE_COLORS,\n",
    "        title=f'Per-Cluster Ho Type Distribution \\u2014 {run_name}',\n",
    "        filename=f'type_distribution_{safe_name}.png',\n",
    "        max_clusters=20 if run_info['has_noise'] else 40,\n",
    "        show_noise=run_info['has_noise'],\n",
    "    )\n",
    "    type_dist_results[run_name] = ct_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf075520",
   "metadata": {},
   "source": [
    "### Dominant Type per Cluster\n",
    "\n",
    "As a complement to the heatmaps, we print each cluster's **dominant type** (the Ho type\n",
    "with the highest proportion) and its **purity** (that proportion). A purity of 1.0 means\n",
    "every indicator in the cluster has the same primary Ho label; a purity of 0.3 means the\n",
    "cluster is a mixture with no single type dominating.\n",
    "\n",
    "This is a quick summary for comparing runs: higher average purity means the clustering\n",
    "better separates wordplay types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef964fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name, ct_norm in type_dist_results.items():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'{run_name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    for idx in ct_norm.index:\n",
    "        row = ct_norm.loc[idx]\n",
    "        dominant_type = row.idxmax()\n",
    "        purity = row.max()\n",
    "        print(f'  Cluster {str(idx):>6s}: dominant={dominant_type:<12s} purity={purity:.2f}')\n",
    "\n",
    "    # Average purity (excluding noise row if present)\n",
    "    numeric_rows = ct_norm.loc[ct_norm.index != 'noise']\n",
    "    avg_purity = numeric_rows.max(axis=1).mean()\n",
    "    print(f'\\n  Average cluster purity: {avg_purity:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcb5b6",
   "metadata": {},
   "source": [
    "### Save Section 2 Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the per-cluster type distribution tables for use in NB 06\n",
    "for run_name, ct_norm in type_dist_results.items():\n",
    "    safe_name = run_name.lower().replace(' ', '_').replace('=', '').replace('.', '')\n",
    "    out_path = OUTPUT_DIR / f'type_distribution_{safe_name}.csv'\n",
    "    ct_norm.to_csv(out_path)\n",
    "    print(f'Saved: {out_path}')\n",
    "\n",
    "# List all figures produced in this section\n",
    "print(f'\\nFigures saved to {FIGURES_DIR}:')\n",
    "for f in sorted(FIGURES_DIR.glob('overlay_*.png')):\n",
    "    print(f'  {f.name}')\n",
    "for f in sorted(FIGURES_DIR.glob('type_distribution_*.png')):\n",
    "    print(f'  {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bf878",
   "metadata": {},
   "source": [
    "### Interpretation: Do Unconstrained Clusters Correspond to Wordplay Types?\n",
    "\n",
    "#### Type Overlay Findings\n",
    "\n",
    "The per-type overlay plots reveal the spatial distribution of each wordplay type in\n",
    "the embedding space. Key observations:\n",
    "\n",
    "- **Homophone** indicators are expected to be the most spatially concentrated — words\n",
    "  like \"sounds like\", \"I hear\", and \"reportedly\" share a distinctive hearing/speaking\n",
    "  semantic field that is well-separated from other types.\n",
    "- **Anagram** indicators are expected to be the most dispersed — they span many\n",
    "  conceptual metaphors (disorder, cooking, damage, movement) and occupy a large region\n",
    "  of the space. This is consistent with anagram having the largest and most diverse\n",
    "  indicator vocabulary (6,610 unique verified indicators).\n",
    "- **Container and insertion** indicators are expected to heavily overlap in space,\n",
    "  since they share placement/containment conceptual metaphors and many of the same\n",
    "  indicator phrases (\"in\", \"about\", \"around\" appear under both types).\n",
    "- **Reversal** indicators likely form a moderately concentrated region, especially the\n",
    "  up/rising words (for down clues) and the back/return words.\n",
    "- **Hidden** indicators may partially overlap with container/insertion due to shared\n",
    "  placement metaphors (\"in\", \"within\", \"inside\" are shared across these types).\n",
    "\n",
    "The GT overlay confirms the spatial patterns using the higher-precision algorithmic\n",
    "labels, covering only the 4 types where mechanical verification is possible.\n",
    "\n",
    "#### Per-Cluster Distribution Findings\n",
    "\n",
    "The heatmaps answer the core question of whether unconstrained clusters align with\n",
    "wordplay types:\n",
    "\n",
    "- **At k=8**: If clusters perfectly aligned with types, each row would have a single\n",
    "  bright cell and near-zero elsewhere. In practice, most clusters are likely mixed —\n",
    "  especially those spanning the container/insertion/hidden region.\n",
    "- **At k=10**: The local silhouette optimum from NB 04. Compare the purity of the k=10\n",
    "  heatmap to k=8 to see whether the two extra clusters help isolate overlapping types.\n",
    "- **At k=34**: Finer granularity should produce purer clusters, but now multiple\n",
    "  clusters correspond to the same type. This is consistent with the NB 04 finding that\n",
    "  the data's natural structure is finer-grained than 8 types.\n",
    "- **HDBSCAN eps=0.0**: The 282 tight clusters are very fine-grained. The top 20 largest\n",
    "  clusters likely show high type purity — each tight cluster contains indicators of\n",
    "  mostly one type. The noise row reveals which types have the most \"ambiguous\" indicators\n",
    "  that don't fit neatly into any dense region.\n",
    "\n",
    "#### Key Takeaway\n",
    "\n",
    "The alignment between unconstrained clusters and wordplay types establishes the baseline\n",
    "for Section 3, where we ask: **can domain knowledge (seed words, constraints) improve\n",
    "this alignment?** If the unconstrained clusters already separate types well for some\n",
    "types (e.g., homophone, reversal) but not others (e.g., container/insertion/hidden),\n",
    "that tells us exactly where constrained clustering has room to help — and where the\n",
    "linguistic reality of shared indicator vocabulary may make clean separation impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f362013",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Constrained Agglomerative Clustering — to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac581cea",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Subset Experiments — to be added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ccc)",
   "language": "python",
   "name": "crossword"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
