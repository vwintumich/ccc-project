{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stage 4: Clustering\n",
    "\n",
    "**Primary author:** Victoria\n",
    "**Builds on:**\n",
    "- *Hierarchical_Clustering_Indicators_with_BGE_M3_Embeddings.ipynb* (Victoria — HDBSCAN; Sahana — agglomerative/Ward's)\n",
    "- *NC_Indicators_wo_Context_Clustered_Embeddings_Compared.ipynb* (Nathan — DBSCAN, EmbeddingComparison framework, silhouette evaluation)\n",
    "- `03_dimensionality_reduction.ipynb` (Stage 3 output: UMAP embeddings)\n",
    "\n",
    "**Prompt engineering:** Victoria\n",
    "**AI assistance:** Claude (Anthropic)\n",
    "**Environment:** Great Lakes for parameter sweeps, Colab for single runs\n",
    "\n",
    "This notebook applies unsupervised clustering to the 10-dimensional UMAP embeddings\n",
    "of 12,622 verified CCC indicators. Two clustering approaches are used:\n",
    "\n",
    "1. **HDBSCAN** — a density-based method that discovers clusters of varying density\n",
    "   and automatically determines the number of clusters. Requires an epsilon\n",
    "   sensitivity analysis (per KCT, Feb 15).\n",
    "2. **Agglomerative clustering with Ward's method** — a hierarchical method that\n",
    "   merges clusters bottom-up using global variance statistics. Requires a\n",
    "   predetermined number of clusters (k), which we set to theory-motivated values\n",
    "   derived from expert CCC references.\n",
    "\n",
    "The notebook follows a principled workflow:\n",
    "- Analyze the pairwise distance distribution to understand the data's distance scale\n",
    "- Use that distribution to select epsilon candidates for HDBSCAN\n",
    "- Run a sensitivity analysis across epsilon values\n",
    "- Compare HDBSCAN results to agglomerative clustering at several granularities\n",
    "- Visualize and qualitatively inspect the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-note",
   "metadata": {},
   "source": [
    "## Running on Google Colab\n",
    "\n",
    "If running on Google Colab:\n",
    "\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. A GPU is not required for clustering, but will speed up UMAP if you need to\n",
    "   regenerate embeddings\n",
    "3. Click **Save**, then run all cells\n",
    "\n",
    "For parameter sweeps (many HDBSCAN runs), Great Lakes is recommended.\n",
    "Single clustering runs complete in seconds on any hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import (\n    silhouette_score,\n    davies_bouldin_score,\n    calinski_harabasz_score,\n)\nfrom scipy.spatial.distance import pdist\n\nimport hdbscan  # install via: pip install hdbscan\n\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nmatplotlib.rcParams['figure.dpi'] = 120\n\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "### Environment Auto-Detection and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Auto-Detection ---\n",
    "try:\n",
    "    IS_COLAB = 'google.colab' in str(get_ipython())\n",
    "except NameError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues')\n",
    "else:\n",
    "    try:\n",
    "        PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Data directory: {DATA_DIR}')\n",
    "print(f'Output directory: {OUTPUT_DIR}')\n",
    "print(f'Figures directory: {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "### Input File Validation\n",
    "\n",
    "Before proceeding, we verify that all required input files from earlier stages exist.\n",
    "These are:\n",
    "\n",
    "| File | Produced by | Description |\n",
    "|------|-------------|-------------|\n",
    "| `embeddings_umap_10d.npy` | Stage 3 | 10D UMAP embeddings for clustering |\n",
    "| `embeddings_umap_2d.npy` | Stage 3 | 2D UMAP embeddings for visualization |\n",
    "| `indicator_index_all.csv` | Stage 2 | Row-to-indicator-string mapping |\n",
    "| `verified_clues_labeled.csv` | Stage 1 | Clue-indicator pairs with wordplay labels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_files = {\n",
    "    'embeddings_umap_10d.npy': 'Run 03_dimensionality_reduction.ipynb (Stage 3)',\n",
    "    'embeddings_umap_2d.npy': 'Run 03_dimensionality_reduction.ipynb (Stage 3)',\n",
    "    'indicator_index_all.csv': 'Run 02_embedding_generation.ipynb (Stage 2)',\n",
    "    'verified_clues_labeled.csv': 'Run 01_data_cleaning.ipynb (Stage 1)',\n",
    "}\n",
    "\n",
    "for fname, fix_msg in required_files.items():\n",
    "    fpath = DATA_DIR / fname\n",
    "    assert fpath.exists(), (\n",
    "        f'Missing required file: {fpath}\\n'\n",
    "        f'Fix: {fix_msg}'\n",
    "    )\n",
    "\n",
    "print('All input files found:')\n",
    "for fname in required_files:\n",
    "    print(f'  {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "### Load Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UMAP embeddings\n",
    "embeddings_10d = np.load(DATA_DIR / 'embeddings_umap_10d.npy')\n",
    "embeddings_2d = np.load(DATA_DIR / 'embeddings_umap_2d.npy')\n",
    "\n",
    "# Load indicator index (maps row i -> indicator string)\n",
    "df_index = pd.read_csv(DATA_DIR / 'indicator_index_all.csv', index_col=0)\n",
    "\n",
    "# Load clue-level labels\n",
    "df_labels = pd.read_csv(DATA_DIR / 'verified_clues_labeled.csv')\n",
    "\n",
    "print(f'10D embeddings shape: {embeddings_10d.shape}')\n",
    "print(f'2D embeddings shape:  {embeddings_2d.shape}')\n",
    "print(f'Indicator index rows: {len(df_index)}')\n",
    "print(f'Clue-label rows:      {len(df_labels)}')\n",
    "\n",
    "# Sanity checks\n",
    "n_indicators = len(df_index)\n",
    "assert embeddings_10d.shape == (n_indicators, 10), (\n",
    "    f'Expected 10D shape ({n_indicators}, 10), got {embeddings_10d.shape}'\n",
    ")\n",
    "assert embeddings_2d.shape == (n_indicators, 2), (\n",
    "    f'Expected 2D shape ({n_indicators}, 2), got {embeddings_2d.shape}'\n",
    ")\n",
    "print(f'\\nShape checks passed: {n_indicators:,} indicators')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-explanation",
   "metadata": {},
   "source": [
    "### Build Per-Indicator Label Sets\n",
    "\n",
    "The label file `verified_clues_labeled.csv` contains one row per (clue_id, indicator)\n",
    "pair — 76,015 rows total. Each row has two independent label columns:\n",
    "\n",
    "- **`wordplay_ho`** — the wordplay type from George Ho's blog parsing. Covers all 8\n",
    "  types (anagram, reversal, hidden, container, insertion, deletion, homophone,\n",
    "  alternation). Available for every row.\n",
    "- **`wordplay_gt`** — algorithmically derived ground truth. Checks whether the answer\n",
    "  can be produced by applying the wordplay operation to the clue text (e.g., is the\n",
    "  answer a permutation of the fodder letters? Is the answer hidden consecutively in\n",
    "  the clue?). Covers only 4 types (hidden, reversal, alternation, anagram) and is\n",
    "  null for many rows where no pattern fires or the answer is too short.\n",
    "\n",
    "**These are two independent classification systems and should not be merged.** Ho labels\n",
    "come from human blog commentary; GT labels come from algorithmic pattern matching. They\n",
    "agree 92.6% of the time where GT exists.\n",
    "\n",
    "**Multi-label indicators:** A single indicator string can appear under multiple wordplay\n",
    "types. For example, \"about\" appears as container, reversal, and anagram under Ho labels.\n",
    "This is a genuine linguistic property of CCC indicators — the same word can signal\n",
    "different operations in different clue contexts. We preserve this by collecting the\n",
    "*set* of distinct labels per unique indicator, rather than collapsing to a single\n",
    "\"primary\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indicator column name from the index file\n",
    "indicator_col = df_index.columns[0]  # should be 'indicator'\n",
    "\n",
    "# Group the clue-level labels by indicator to get the set of Ho labels\n",
    "# and the set of GT labels for each unique indicator\n",
    "ho_labels_per_indicator = (\n",
    "    df_labels\n",
    "    .dropna(subset=['wordplay_ho'])\n",
    "    .groupby('indicator')['wordplay_ho']\n",
    "    .apply(lambda x: set(x.unique()))\n",
    "    .rename('ho_label_set')\n",
    ")\n",
    "\n",
    "gt_labels_per_indicator = (\n",
    "    df_labels\n",
    "    .dropna(subset=['wordplay_gt'])\n",
    "    .groupby('indicator')['wordplay_gt']\n",
    "    .apply(lambda x: set(x.unique()))\n",
    "    .rename('gt_label_set')\n",
    ")\n",
    "\n",
    "# Join onto the indicator index\n",
    "df_indicators = df_index.copy()\n",
    "df_indicators = df_indicators.merge(\n",
    "    ho_labels_per_indicator, left_on=indicator_col, right_index=True, how='left'\n",
    ")\n",
    "df_indicators = df_indicators.merge(\n",
    "    gt_labels_per_indicator, left_on=indicator_col, right_index=True, how='left'\n",
    ")\n",
    "\n",
    "# For indicators with no Ho labels in the labeled file, fill with empty set\n",
    "df_indicators['ho_label_set'] = df_indicators['ho_label_set'].apply(\n",
    "    lambda x: x if isinstance(x, set) else set()\n",
    ")\n",
    "df_indicators['gt_label_set'] = df_indicators['gt_label_set'].apply(\n",
    "    lambda x: x if isinstance(x, set) else set()\n",
    ")\n",
    "\n",
    "# Count how many Ho labels each indicator has\n",
    "df_indicators['n_ho_labels'] = df_indicators['ho_label_set'].apply(len)\n",
    "\n",
    "print(f'Indicators with Ho labels: {(df_indicators[\"n_ho_labels\"] > 0).sum():,}')\n",
    "print(f'Indicators with no Ho labels: {(df_indicators[\"n_ho_labels\"] == 0).sum():,}')\n",
    "print(f'\\nMulti-label distribution:')\n",
    "print(df_indicators['n_ho_labels'].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-ho-label-explanation",
   "metadata": {},
   "source": [
    "For visualization purposes, we also create a \"primary Ho label\" column that picks the\n",
    "single most common Ho label for each indicator. This is used **only** for coloring\n",
    "scatter plots where each point needs a single color. The full label set\n",
    "(`ho_label_set`) is used for all quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scatter plot coloring: pick the most frequent Ho label per indicator.\n",
    "# When an indicator has multiple Ho labels (e.g., \"about\" = container, reversal,\n",
    "# anagram), we pick the one that appears in the most clues for that indicator.\n",
    "primary_ho = (\n",
    "    df_labels\n",
    "    .dropna(subset=['wordplay_ho'])\n",
    "    .groupby(['indicator', 'wordplay_ho'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    "    .drop_duplicates(subset='indicator', keep='first')\n",
    "    .set_index('indicator')['wordplay_ho']\n",
    "    .rename('primary_ho_label')\n",
    ")\n",
    "\n",
    "df_indicators = df_indicators.merge(\n",
    "    primary_ho, left_on=indicator_col, right_index=True, how='left'\n",
    ")\n",
    "\n",
    "print('Primary Ho label distribution:')\n",
    "print(df_indicators['primary_ho_label'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Pairwise Distance Analysis\n",
    "\n",
    "**Why this step is necessary:** HDBSCAN's `cluster_selection_epsilon` parameter defines\n",
    "a distance threshold below which clusters will be merged. If we choose an epsilon of 0.5\n",
    "but all pairwise distances in our data are between 0.01 and 0.05, that epsilon is\n",
    "meaninglessly large — it would merge everything into one cluster. Conversely, an epsilon\n",
    "of 0.001 in that same data would have no effect. **Without knowing the actual distance\n",
    "scale, any epsilon choice is arbitrary.**\n",
    "\n",
    "Following KCT's guidance (Feb 15 meeting): \"Take a look at typical pairwise distances,\n",
    "get a distribution, pick a value of epsilon based on that.\"\n",
    "\n",
    "We compute pairwise Euclidean distances on a random sample of 2,000 points from the\n",
    "10D UMAP embeddings. Using all 12,622 points would produce ~80 million distance pairs\n",
    "(12,622 × 12,621 / 2), which is computationally expensive. A sample of 2,000 gives\n",
    "~2 million pairs — more than enough to characterize the distribution.\n",
    "\n",
    "**Note on distance metric:** HDBSCAN uses Euclidean distance by default, and our 10D\n",
    "UMAP embeddings were produced with `metric='cosine'` — meaning UMAP already arranged\n",
    "the points so that cosine-similar items are Euclidean-close in the reduced space. So\n",
    "Euclidean distance in UMAP space is appropriate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pairwise-distances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2,000 points for pairwise distance computation\n",
    "sample_size = 2000\n",
    "rng = np.random.RandomState(42)\n",
    "sample_idx = rng.choice(len(embeddings_10d), size=sample_size, replace=False)\n",
    "sample_embeddings = embeddings_10d[sample_idx]\n",
    "\n",
    "# Compute pairwise Euclidean distances (returns condensed form)\n",
    "# pdist returns a 1D array of all unique pairs\n",
    "pairwise_dists = pdist(sample_embeddings, metric='euclidean')\n",
    "\n",
    "print(f'Sample size: {sample_size}')\n",
    "print(f'Number of pairwise distances: {len(pairwise_dists):,}')\n",
    "print(f'\\nDistance statistics:')\n",
    "print(f'  Min:    {pairwise_dists.min():.4f}')\n",
    "print(f'  Max:    {pairwise_dists.max():.4f}')\n",
    "print(f'  Mean:   {pairwise_dists.mean():.4f}')\n",
    "print(f'  Median: {np.median(pairwise_dists):.4f}')\n",
    "print(f'  Std:    {pairwise_dists.std():.4f}')\n",
    "\n",
    "# Key percentiles — these will guide epsilon selection\n",
    "percentiles = [5, 10, 25, 50, 75, 90, 95]\n",
    "percentile_values = np.percentile(pairwise_dists, percentiles)\n",
    "\n",
    "print(f'\\nKey percentiles:')\n",
    "for p, v in zip(percentiles, percentile_values):\n",
    "    print(f'  {p:3d}th percentile: {v:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dist-plot-explanation",
   "metadata": {},
   "source": [
    "### Distance Distribution Histogram\n",
    "\n",
    "The histogram below shows the distribution of all pairwise Euclidean distances in the\n",
    "2,000-point sample. Vertical lines mark key percentiles. The shape of this distribution\n",
    "tells us:\n",
    "\n",
    "- **If unimodal and tight:** Most points are at similar distances — the data is\n",
    "  relatively uniform, and clustering will be sensitive to epsilon.\n",
    "- **If multimodal or with a long tail:** There are distinct distance scales — some\n",
    "  points are much closer to each other than to the bulk, suggesting natural clusters.\n",
    "- **Where the bulk of the distribution sits:** This sets the scale for epsilon selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dist-histogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.hist(pairwise_dists, bins=200, color='steelblue', alpha=0.7, edgecolor='none',\n",
    "        density=True)\n",
    "\n",
    "# Mark key percentiles\n",
    "colors_pctl = ['#e41a1c', '#ff7f00', '#4daf4a', '#377eb8', '#4daf4a', '#ff7f00', '#e41a1c']\n",
    "for p, v, c in zip(percentiles, percentile_values, colors_pctl):\n",
    "    ax.axvline(v, color=c, linestyle='--', alpha=0.7, linewidth=1.2,\n",
    "               label=f'{p}th pctl = {v:.3f}')\n",
    "\n",
    "ax.set_xlabel('Euclidean Distance (10D UMAP space)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Pairwise Distance Distribution (2,000-point sample)')\n",
    "ax.legend(fontsize=8, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'pairwise_distance_distribution.png', dpi=150,\n",
    "            bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved: {FIGURES_DIR / \"pairwise_distance_distribution.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: HDBSCAN with Epsilon Sensitivity Analysis\n",
    "\n",
    "### What is HDBSCAN?\n",
    "\n",
    "**HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)**\n",
    "is a density-based clustering algorithm. Unlike k-means, it does not require the number\n",
    "of clusters to be specified in advance — it discovers clusters of varying density\n",
    "automatically. Points in low-density regions are labeled as \"noise\" (cluster label = -1)\n",
    "rather than being forced into the nearest cluster.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **`min_cluster_size`** — The minimum number of points required to form a cluster.\n",
    "  We use 10, meaning any group of fewer than 10 nearby indicators is treated as noise\n",
    "  rather than a cluster. This prevents tiny, meaningless clusters.\n",
    "\n",
    "- **`cluster_selection_epsilon`** — A distance threshold for cluster merging. When two\n",
    "  clusters are closer than epsilon, HDBSCAN merges them into one. Larger epsilon values\n",
    "  produce fewer, larger clusters; smaller values produce more, finer-grained clusters.\n",
    "  **This is the parameter we will sweep** across values informed by the distance\n",
    "  distribution in Section 2.\n",
    "\n",
    "### Interpreting the Sensitivity Analysis\n",
    "\n",
    "We run HDBSCAN at several epsilon values and track:\n",
    "- **Number of clusters found** — How granularity changes with epsilon\n",
    "- **Noise percentage** — How many points are left unassigned\n",
    "- **Silhouette score** — Measures how similar each point is to its own cluster vs.\n",
    "  the nearest other cluster (-1 to 1; higher is better). Computed on non-noise\n",
    "  points only.\n",
    "- **Davies-Bouldin index** — Measures the average similarity between each cluster\n",
    "  and its most similar cluster (lower is better).\n",
    "\n",
    "**Stability interpretation:** If metrics change dramatically with small epsilon changes,\n",
    "the cluster structure is **fragile** — it depends sensitively on a parameter choice\n",
    "rather than reflecting genuine structure. If metrics are stable across a range of epsilon\n",
    "values, the structure is **robust** and we can be more confident in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epsilon-selection",
   "metadata": {},
   "source": [
    "### Select Epsilon Candidates\n",
    "\n",
    "We choose epsilon candidates based on the distance distribution percentiles computed\n",
    "above. The candidates span from a small value (which will produce many fine-grained\n",
    "clusters) to a larger value (which will merge clusters aggressively). We also include\n",
    "0.0 as a baseline (no merging — pure HDBSCAN without epsilon smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epsilon-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select epsilon candidates from the distance distribution\n",
    "# We use percentiles of the pairwise distance distribution as a principled basis.\n",
    "# The candidates range from 0.0 (no epsilon merging) to around the 25th percentile.\n",
    "# Values beyond the median would merge nearly everything.\n",
    "epsilon_candidates = [\n",
    "    0.0,                                            # baseline: no epsilon merging\n",
    "    float(np.percentile(pairwise_dists, 1)),        # ~1st percentile\n",
    "    float(np.percentile(pairwise_dists, 5)),        # ~5th percentile\n",
    "    float(np.percentile(pairwise_dists, 10)),       # ~10th percentile\n",
    "    float(np.percentile(pairwise_dists, 15)),       # ~15th percentile\n",
    "    float(np.percentile(pairwise_dists, 20)),       # ~20th percentile\n",
    "    float(np.percentile(pairwise_dists, 25)),       # ~25th percentile\n",
    "]\n",
    "\n",
    "# Round for cleaner display\n",
    "epsilon_candidates = [round(e, 4) for e in epsilon_candidates]\n",
    "\n",
    "print('Epsilon candidates for HDBSCAN sweep:')\n",
    "for i, eps in enumerate(epsilon_candidates):\n",
    "    print(f'  {i+1}. epsilon = {eps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdbscan-sweep-explanation",
   "metadata": {},
   "source": [
    "### Run HDBSCAN Sweep\n",
    "\n",
    "For each epsilon value, we run HDBSCAN on the 10D UMAP embeddings and record\n",
    "clustering metrics. Silhouette and Davies-Bouldin scores are computed on non-noise\n",
    "points only (since noise points have no cluster assignment). This is standard practice\n",
    "but means that a run with very high noise (few clustered points) can have a deceptively\n",
    "high silhouette score — we must examine noise percentage alongside silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_results = []\n",
    "hdbscan_labels_dict = {}  # store labels for each epsilon\n",
    "\n",
    "for eps in epsilon_candidates:\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=10,\n",
    "        cluster_selection_epsilon=eps,\n",
    "        metric='euclidean',\n",
    "        core_dist_n_jobs=-1,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(embeddings_10d)\n",
    "    hdbscan_labels_dict[eps] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels == -1).sum()\n",
    "    noise_pct = n_noise / len(labels) * 100\n",
    "\n",
    "    # Compute metrics on non-noise points only\n",
    "    non_noise_mask = labels != -1\n",
    "    n_clustered = non_noise_mask.sum()\n",
    "\n",
    "    if n_clusters >= 2 and n_clustered >= 2:\n",
    "        sil = silhouette_score(embeddings_10d[non_noise_mask], labels[non_noise_mask])\n",
    "        db = davies_bouldin_score(embeddings_10d[non_noise_mask], labels[non_noise_mask])\n",
    "    else:\n",
    "        sil = float('nan')\n",
    "        db = float('nan')\n",
    "\n",
    "    hdbscan_results.append({\n",
    "        'epsilon': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'noise_pct': noise_pct,\n",
    "        'n_clustered': n_clustered,\n",
    "        'silhouette': sil,\n",
    "        'davies_bouldin': db,\n",
    "    })\n",
    "\n",
    "    print(f'eps={eps:.4f}: {n_clusters:4d} clusters, '\n",
    "          f'{n_noise:5d} noise ({noise_pct:5.1f}%), '\n",
    "          f'silhouette={sil:.3f}, DB={db:.3f}')\n",
    "\n",
    "df_hdbscan = pd.DataFrame(hdbscan_results)\n",
    "print('\\n--- HDBSCAN Sweep Summary ---')\n",
    "print(df_hdbscan.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitivity-plot-explanation",
   "metadata": {},
   "source": [
    "### Sensitivity Plot\n",
    "\n",
    "The plot below shows how three key metrics change as epsilon increases. This is the\n",
    "sensitivity analysis KCT requested. We look for:\n",
    "\n",
    "- **Stable regions:** Where metrics plateau, indicating robust structure\n",
    "- **Sharp transitions:** Where a small epsilon change causes large metric jumps,\n",
    "  indicating fragile structure at that scale\n",
    "- **Trade-offs:** More clusters typically means more noise points but potentially\n",
    "  higher silhouette (tighter individual clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitivity-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Number of clusters\n",
    "axes[0].plot(df_hdbscan['epsilon'], df_hdbscan['n_clusters'],\n",
    "             'o-', color='steelblue', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Epsilon')\n",
    "axes[0].set_ylabel('Number of Clusters')\n",
    "axes[0].set_title('Clusters Found')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Noise percentage\n",
    "axes[1].plot(df_hdbscan['epsilon'], df_hdbscan['noise_pct'],\n",
    "             'o-', color='#e41a1c', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epsilon')\n",
    "axes[1].set_ylabel('Noise Points (%)')\n",
    "axes[1].set_title('Noise Percentage')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Silhouette score\n",
    "axes[2].plot(df_hdbscan['epsilon'], df_hdbscan['silhouette'],\n",
    "             'o-', color='#4daf4a', linewidth=2, markersize=6)\n",
    "axes[2].set_xlabel('Epsilon')\n",
    "axes[2].set_ylabel('Silhouette Score')\n",
    "axes[2].set_title('Silhouette Score (non-noise only)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('HDBSCAN Epsilon Sensitivity Analysis', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'hdbscan_epsilon_sensitivity.png', dpi=150,\n",
    "            bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved: {FIGURES_DIR / \"hdbscan_epsilon_sensitivity.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-hdbscan-header",
   "metadata": {},
   "source": [
    "### Identify Best HDBSCAN Run\n",
    "\n",
    "We select the HDBSCAN run with the highest silhouette score as the \"best\" run for\n",
    "visualization and qualitative inspection later in this notebook. This is a pragmatic\n",
    "choice — silhouette score is not the only criterion, and we save all runs so that\n",
    "Notebook 05 can compare them more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-hdbscan",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hdbscan_idx = df_hdbscan['silhouette'].idxmax()\n",
    "best_hdbscan_row = df_hdbscan.loc[best_hdbscan_idx]\n",
    "best_eps = best_hdbscan_row['epsilon']\n",
    "best_hdbscan_labels = hdbscan_labels_dict[best_eps]\n",
    "\n",
    "print(f'Best HDBSCAN run by silhouette score:')\n",
    "print(f'  Epsilon:    {best_eps}')\n",
    "print(f'  Clusters:   {int(best_hdbscan_row[\"n_clusters\"])}')\n",
    "print(f'  Noise:      {int(best_hdbscan_row[\"n_noise\"])} ({best_hdbscan_row[\"noise_pct\"]:.1f}%)')\n",
    "print(f'  Silhouette: {best_hdbscan_row[\"silhouette\"]:.3f}')\n",
    "print(f'  Davies-Bouldin: {best_hdbscan_row[\"davies_bouldin\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Agglomerative Clustering with Ward's Method\n",
    "\n",
    "### Why Ward's Method?\n",
    "\n",
    "**Agglomerative clustering** is a bottom-up hierarchical method: each point starts as\n",
    "its own cluster, and pairs of clusters are merged iteratively until the desired number\n",
    "of clusters (k) is reached.\n",
    "\n",
    "**Ward's linkage** minimizes the total within-cluster variance at each merge step. As\n",
    "KCT recommended (Feb 15 meeting): \"Use Ward's method. It uses more global statistics\n",
    "about the cluster, so doesn't form strange elongated clusters.\" Unlike single-linkage\n",
    "(which can chain together distant points through intermediate neighbors) or average-\n",
    "linkage (which can produce uneven cluster sizes), Ward's method produces compact,\n",
    "roughly spherical clusters.\n",
    "\n",
    "### Theory-Motivated k Values\n",
    "\n",
    "Unlike HDBSCAN, agglomerative clustering requires the number of clusters (k) to be\n",
    "specified. We choose k values based on the theoretical framework for CCC indicator\n",
    "structure, with each k corresponding to a specific level of the conceptual hierarchy.\n",
    "The source for each k is the `wordplay_seeds.xlsx` spreadsheet, compiled from expert\n",
    "CCC references:\n",
    "\n",
    "| k | Source | Meaning |\n",
    "|---|--------|---------|\n",
    "| 6 | `cc_for_dummies_ho_6` | Cryptic Crosswords for Dummies mapped to 6 of Ho's types |\n",
    "| 8 | Ho's labeled types | All 8 wordplay types in the dataset |\n",
    "| 12 | `minute_cryptic_ho_7` | Minute Cryptic categories with subcategories totaling 12 groups |\n",
    "| 26 | `minute_cryptic_ALL` | Minute Cryptic full subcategories (most granular expert view) |\n",
    "| 34 | `conceptual_groups` | Victoria's two-tier conceptual categories (organized by metaphor) |\n",
    "\n",
    "These are theory-motivated choices, not arbitrary. Each represents a different hypothesis\n",
    "about the natural granularity of indicator language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglo-clustering",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [6, 8, 12, 26, 34]\n",
    "k_descriptions = {\n",
    "    6: 'CC for Dummies -> 6 Ho types',\n",
    "    8: 'All 8 Ho wordplay types',\n",
    "    12: 'Minute Cryptic -> Ho (12 subcategories)',\n",
    "    26: 'Minute Cryptic ALL (26 subcategories)',\n",
    "    34: 'Conceptual groups (34 categories)',\n",
    "}\n",
    "\n",
    "agglo_results = []\n",
    "agglo_labels_dict = {}  # store labels for each k\n",
    "\n",
    "for k in k_values:\n",
    "    clusterer = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        linkage='ward',\n",
    "    )\n",
    "    labels = clusterer.fit_predict(embeddings_10d)\n",
    "    agglo_labels_dict[k] = labels\n",
    "\n",
    "    sil = silhouette_score(embeddings_10d, labels)\n",
    "    db = davies_bouldin_score(embeddings_10d, labels)\n",
    "    ch = calinski_harabasz_score(embeddings_10d, labels)\n",
    "\n",
    "    agglo_results.append({\n",
    "        'k': k,\n",
    "        'description': k_descriptions[k],\n",
    "        'silhouette': sil,\n",
    "        'davies_bouldin': db,\n",
    "        'calinski_harabasz': ch,\n",
    "    })\n",
    "\n",
    "    print(f'k={k:2d} ({k_descriptions[k]:>40s}): '\n",
    "          f'silhouette={sil:.3f}, DB={db:.3f}, CH={ch:.0f}')\n",
    "\n",
    "df_agglo = pd.DataFrame(agglo_results)\n",
    "print('\\n--- Agglomerative Clustering Summary ---')\n",
    "print(df_agglo.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglo-interpretation",
   "metadata": {},
   "source": [
    "### Interpreting the Metrics\n",
    "\n",
    "- **Silhouette score** (-1 to 1): Higher means points are well-matched to their own\n",
    "  cluster and poorly matched to neighboring clusters. Scores above 0.3 suggest\n",
    "  reasonable structure; above 0.5 is strong.\n",
    "- **Davies-Bouldin index** (0 to ∞): Lower is better. Measures the average ratio of\n",
    "  within-cluster scatter to between-cluster separation. A score below 1.0 means\n",
    "  clusters are more separated than they are scattered.\n",
    "- **Calinski-Harabasz index** (0 to ∞): Higher is better. Ratio of between-cluster\n",
    "  dispersion to within-cluster dispersion. Tends to favor larger k values, so\n",
    "  interpret with caution.\n",
    "\n",
    "Note that agglomerative clustering assigns every point to a cluster (no noise points),\n",
    "which makes its silhouette scores directly comparable across k values but not directly\n",
    "comparable to HDBSCAN (which excludes noise points from the calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Cluster Visualization\n",
    "\n",
    "We visualize two clustering runs in detail:\n",
    "1. **Agglomerative k=8** — the baseline hypothesis that indicators cluster into the\n",
    "   8 labeled wordplay types\n",
    "2. **Best HDBSCAN** — the best density-based result by silhouette score\n",
    "\n",
    "For each run, we produce:\n",
    "- A scatter plot of the 2D UMAP projection colored by cluster assignment\n",
    "- Eight scatter plots (one per Ho wordplay type) showing where each type's indicators\n",
    "  landed across the clusters\n",
    "\n",
    "### Why Per-Type Plots?\n",
    "\n",
    "Instead of asking \"what is the correct label for this cluster?\" we ask **\"where did all\n",
    "the anagram indicators end up?\"** This approach examines clustering from the label side\n",
    "and handles multi-label indicators naturally — an indicator like \"about\" (which is\n",
    "container, reversal, and anagram under Ho labels) simply appears highlighted in all\n",
    "three of those type-specific plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-helpers",
   "metadata": {},
   "source": [
    "### Visualization Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 8 Ho wordplay types\n",
    "HO_TYPES = ['anagram', 'reversal', 'hidden', 'container', 'insertion',\n",
    "             'deletion', 'homophone', 'alternation']\n",
    "\n",
    "# Consistent color palette for the 8 Ho wordplay types\n",
    "HO_COLORS = {\n",
    "    'anagram': '#e41a1c',\n",
    "    'reversal': '#377eb8',\n",
    "    'hidden': '#4daf4a',\n",
    "    'container': '#984ea3',\n",
    "    'insertion': '#ff7f00',\n",
    "    'deletion': '#a65628',\n",
    "    'homophone': '#f781bf',\n",
    "    'alternation': '#999999',\n",
    "}\n",
    "\n",
    "\n",
    "def plot_clusters(embeddings_2d, labels, title, filename, noise_label=-1):\n",
    "    \"\"\"Scatter plot of 2D UMAP colored by cluster assignment.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Plot noise points first (gray, small)\n",
    "    noise_mask = labels == noise_label\n",
    "    if noise_mask.any():\n",
    "        ax.scatter(\n",
    "            embeddings_2d[noise_mask, 0],\n",
    "            embeddings_2d[noise_mask, 1],\n",
    "            s=1, alpha=0.15, color='lightgray', label='noise', zorder=1\n",
    "        )\n",
    "\n",
    "    # Plot clustered points\n",
    "    non_noise_mask = ~noise_mask\n",
    "    unique_labels = sorted(set(labels[non_noise_mask]))\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Use a colormap with enough distinct colors\n",
    "    cmap = plt.cm.get_cmap('tab20', max(n_clusters, 20))\n",
    "    for i, cl in enumerate(unique_labels):\n",
    "        mask = labels == cl\n",
    "        ax.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            s=2, alpha=0.4, color=cmap(i % 20), zorder=2\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title(f'{title} ({n_clusters} clusters)')\n",
    "\n",
    "    if noise_mask.any():\n",
    "        n_noise = noise_mask.sum()\n",
    "        ax.annotate(f'Noise: {n_noise} ({n_noise/len(labels)*100:.1f}%)',\n",
    "                    xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                    fontsize=9, va='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Saved: {FIGURES_DIR / filename}')\n",
    "\n",
    "\n",
    "def plot_ho_type_overlay(embeddings_2d, ho_type, df_indicators, indicator_col,\n",
    "                         title_prefix, filename_prefix):\n",
    "    \"\"\"Scatter plot highlighting indicators of a single Ho wordplay type.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # All points in gray\n",
    "    ax.scatter(\n",
    "        embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "        s=1, alpha=0.1, color='lightgray', zorder=1\n",
    "    )\n",
    "\n",
    "    # Highlight indicators that have this Ho label (checking the set, not primary)\n",
    "    mask = df_indicators['ho_label_set'].apply(lambda s: ho_type in s).values\n",
    "    n_highlighted = mask.sum()\n",
    "\n",
    "    ax.scatter(\n",
    "        embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "        s=4, alpha=0.5, color=HO_COLORS[ho_type], zorder=2,\n",
    "        label=f'{ho_type} (n={n_highlighted:,})'\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title(f'{title_prefix} — Ho type: {ho_type}')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "    filename = f'{filename_prefix}_ho_{ho_type}.png'\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Saved: {FIGURES_DIR / filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglo-k8-viz-header",
   "metadata": {},
   "source": [
    "### Agglomerative k=8 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglo-k8-cluster-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings_2d,\n",
    "    agglo_labels_dict[8],\n",
    "    title='Agglomerative Clustering (Ward, k=8)',\n",
    "    filename='agglo_k8_clusters.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglo-k8-ho-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ho_type in HO_TYPES:\n",
    "    plot_ho_type_overlay(\n",
    "        embeddings_2d,\n",
    "        ho_type=ho_type,\n",
    "        df_indicators=df_indicators,\n",
    "        indicator_col=indicator_col,\n",
    "        title_prefix='Agglomerative k=8',\n",
    "        filename_prefix='agglo_k8'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdbscan-best-viz-header",
   "metadata": {},
   "source": [
    "### Best HDBSCAN Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan-best-cluster-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings_2d,\n",
    "    best_hdbscan_labels,\n",
    "    title=f'HDBSCAN (eps={best_eps})',\n",
    "    filename='hdbscan_best_clusters.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan-best-ho-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ho_type in HO_TYPES:\n",
    "    plot_ho_type_overlay(\n",
    "        embeddings_2d,\n",
    "        ho_type=ho_type,\n",
    "        df_indicators=df_indicators,\n",
    "        indicator_col=indicator_col,\n",
    "        title_prefix=f'HDBSCAN (eps={best_eps})',\n",
    "        filename_prefix='hdbscan_best'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Basic Qualitative Inspection\n",
    "\n",
    "For each cluster in the agglomerative k=8 and best HDBSCAN runs, we print:\n",
    "1. The 10 indicators closest to the cluster centroid (in 10D UMAP space)\n",
    "2. The distribution of Ho wordplay types within that cluster\n",
    "\n",
    "This gives a quick view of whether each cluster corresponds to a recognizable wordplay\n",
    "type or is a mixture. Deeper qualitative analysis will be in Notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspection-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_clusters(labels, embeddings_10d, df_indicators, indicator_col,\n",
    "                     method_name, n_examples=10):\n",
    "    \"\"\"Print centroid-nearest indicators and Ho type distribution per cluster.\"\"\"\n",
    "    unique_labels = sorted(set(labels))\n",
    "\n",
    "    for cl in unique_labels:\n",
    "        if cl == -1:\n",
    "            # Summarize noise points briefly\n",
    "            n_noise = (labels == -1).sum()\n",
    "            print(f'\\n{\"=\" * 60}')\n",
    "            print(f'{method_name} — Noise points: {n_noise}')\n",
    "            print(f'{\"=\" * 60}')\n",
    "            continue\n",
    "\n",
    "        mask = labels == cl\n",
    "        cluster_size = mask.sum()\n",
    "        cluster_embeddings = embeddings_10d[mask]\n",
    "        cluster_indicators = df_indicators.loc[mask]\n",
    "\n",
    "        # Compute centroid and find nearest points\n",
    "        centroid = cluster_embeddings.mean(axis=0)\n",
    "        dists_to_centroid = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "        nearest_idx = np.argsort(dists_to_centroid)[:n_examples]\n",
    "        nearest_indicators = cluster_indicators.iloc[nearest_idx][indicator_col].tolist()\n",
    "\n",
    "        # Ho type distribution within this cluster\n",
    "        # Explode the label sets so each (indicator, type) pair is counted\n",
    "        ho_exploded = cluster_indicators['ho_label_set'].explode()\n",
    "        ho_exploded = ho_exploded[ho_exploded.apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "        print(f'\\n{\"=\" * 60}')\n",
    "        print(f'{method_name} — Cluster {cl} (n={cluster_size})')\n",
    "        print(f'{\"-\" * 60}')\n",
    "        print(f'Nearest to centroid: {\", \".join(nearest_indicators)}')\n",
    "\n",
    "        if len(ho_exploded) > 0:\n",
    "            type_counts = ho_exploded.value_counts()\n",
    "            total_labels = type_counts.sum()\n",
    "            print(f'Ho type distribution ({total_labels} total label instances):')\n",
    "            for wtype, count in type_counts.items():\n",
    "                pct = count / total_labels * 100\n",
    "                print(f'  {wtype:15s}: {count:5d} ({pct:5.1f}%)')\n",
    "        else:\n",
    "            print('  No Ho labels available for indicators in this cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglo-k8-inspect-header",
   "metadata": {},
   "source": [
    "### Agglomerative k=8 — Cluster Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglo-k8-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_clusters(\n",
    "    agglo_labels_dict[8],\n",
    "    embeddings_10d,\n",
    "    df_indicators,\n",
    "    indicator_col,\n",
    "    method_name='Agglomerative k=8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdbscan-best-inspect-header",
   "metadata": {},
   "source": [
    "### Best HDBSCAN — Cluster Inspection\n",
    "\n",
    "HDBSCAN may produce many clusters. We inspect the 15 largest clusters to keep\n",
    "output manageable. A full inspection of all clusters belongs in Notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan-best-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HDBSCAN, inspect only the largest clusters to keep output concise\n",
    "unique_hdbscan_labels = sorted(set(best_hdbscan_labels))\n",
    "n_hdbscan_clusters = len([l for l in unique_hdbscan_labels if l != -1])\n",
    "\n",
    "if n_hdbscan_clusters > 15:\n",
    "    # Find the 15 largest clusters by size\n",
    "    cluster_sizes = pd.Series(best_hdbscan_labels[best_hdbscan_labels != -1]).value_counts()\n",
    "    top_15_labels = set(cluster_sizes.head(15).index)\n",
    "\n",
    "    # Create a filtered label array: keep top 15, set rest to -1\n",
    "    filtered_labels = np.where(\n",
    "        np.isin(best_hdbscan_labels, list(top_15_labels)),\n",
    "        best_hdbscan_labels,\n",
    "        -1\n",
    "    )\n",
    "    print(f'HDBSCAN found {n_hdbscan_clusters} clusters. '\n",
    "          f'Showing the 15 largest below.')\n",
    "    inspect_clusters(\n",
    "        filtered_labels,\n",
    "        embeddings_10d,\n",
    "        df_indicators,\n",
    "        indicator_col,\n",
    "        method_name=f'HDBSCAN (eps={best_eps})'\n",
    "    )\n",
    "else:\n",
    "    inspect_clusters(\n",
    "        best_hdbscan_labels,\n",
    "        embeddings_10d,\n",
    "        df_indicators,\n",
    "        indicator_col,\n",
    "        method_name=f'HDBSCAN (eps={best_eps})'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Save All Outputs\n",
    "\n",
    "We save cluster labels for every run so that Notebook 05 can load and evaluate them\n",
    "without rerunning the clustering. Each CSV has columns `indicator` and `cluster_label`.\n",
    "\n",
    "**Output files:**\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `cluster_labels_hdbscan_eps_{value}.csv` | `DATA_DIR` | Labels per HDBSCAN epsilon run |\n",
    "| `cluster_labels_agglo_k{value}.csv` | `DATA_DIR` | Labels per agglomerative k |\n",
    "| `clustering_metrics_summary.csv` | `OUTPUT_DIR` | All metrics from all runs |\n",
    "| `figures/*.png` | `OUTPUT_DIR/figures` | All visualizations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-hdbscan-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HDBSCAN cluster labels for each epsilon\n",
    "for eps, labels in hdbscan_labels_dict.items():\n",
    "    eps_str = f'{eps:.4f}'.replace('.', 'p')  # e.g., 0.1500 -> 0p1500\n",
    "    fname = f'cluster_labels_hdbscan_eps_{eps_str}.csv'\n",
    "    out_df = pd.DataFrame({\n",
    "        'indicator': df_indicators[indicator_col].values,\n",
    "        'cluster_label': labels,\n",
    "    })\n",
    "    out_df.to_csv(DATA_DIR / fname, index=False)\n",
    "    print(f'Saved: {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-agglo-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save agglomerative cluster labels for each k\n",
    "for k, labels in agglo_labels_dict.items():\n",
    "    fname = f'cluster_labels_agglo_k{k}.csv'\n",
    "    out_df = pd.DataFrame({\n",
    "        'indicator': df_indicators[indicator_col].values,\n",
    "        'cluster_label': labels,\n",
    "    })\n",
    "    out_df.to_csv(DATA_DIR / fname, index=False)\n",
    "    print(f'Saved: {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a combined metrics summary for all runs\n",
    "all_metrics = []\n",
    "\n",
    "# HDBSCAN runs\n",
    "for _, row in df_hdbscan.iterrows():\n",
    "    all_metrics.append({\n",
    "        'method': 'HDBSCAN',\n",
    "        'parameters': f'min_cluster_size=10, eps={row[\"epsilon\"]}',\n",
    "        'n_clusters': int(row['n_clusters']),\n",
    "        'n_noise': int(row['n_noise']),\n",
    "        'noise_pct': row['noise_pct'],\n",
    "        'silhouette': row['silhouette'],\n",
    "        'davies_bouldin': row['davies_bouldin'],\n",
    "        'calinski_harabasz': float('nan'),  # not computed for HDBSCAN (noise points)\n",
    "    })\n",
    "\n",
    "# Agglomerative runs\n",
    "for _, row in df_agglo.iterrows():\n",
    "    all_metrics.append({\n",
    "        'method': 'Agglomerative (Ward)',\n",
    "        'parameters': f'k={int(row[\"k\"])}',\n",
    "        'n_clusters': int(row['k']),\n",
    "        'n_noise': 0,\n",
    "        'noise_pct': 0.0,\n",
    "        'silhouette': row['silhouette'],\n",
    "        'davies_bouldin': row['davies_bouldin'],\n",
    "        'calinski_harabasz': row['calinski_harabasz'],\n",
    "    })\n",
    "\n",
    "df_all_metrics = pd.DataFrame(all_metrics)\n",
    "metrics_path = OUTPUT_DIR / 'clustering_metrics_summary.csv'\n",
    "df_all_metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f'Saved: {metrics_path}')\n",
    "print(f'\\n--- Full Metrics Summary ---')\n",
    "print(df_all_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final file listing\n",
    "print('=== All Output Files ===')\n",
    "print(f'\\nCluster labels (in {DATA_DIR}):')\n",
    "for f in sorted(DATA_DIR.glob('cluster_labels_*.csv')):\n",
    "    print(f'  {f.name}')\n",
    "\n",
    "print(f'\\nMetrics summary (in {OUTPUT_DIR}):')\n",
    "print(f'  clustering_metrics_summary.csv')\n",
    "\n",
    "print(f'\\nFigures (in {FIGURES_DIR}):')\n",
    "for f in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f'  {f.name}')\n",
    "\n",
    "print('\\nDone. All outputs saved for Notebook 05.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}