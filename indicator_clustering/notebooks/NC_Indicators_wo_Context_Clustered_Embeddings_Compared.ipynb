{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36879,"status":"ok","timestamp":1770520809931,"user":{"displayName":"Nathan Cantwell","userId":"15236319868573600434"},"user_tz":480},"id":"VLmL-0jnWAnb","outputId":"846e0086-20a7-4b21-b280-91f0d5e8c311"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import string\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from sentence_transformers import SentenceTransformer\n","from sklearn.preprocessing import normalize\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score\n","import seaborn as sns\n","from sklearn.cluster import KMeans, DBSCAN, HDBSCAN\n","from sklearn.manifold import TSNE\n","import umap\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":795,"status":"ok","timestamp":1770520810730,"user":{"displayName":"Nathan Cantwell","userId":"15236319868573600434"},"user_tz":480},"id":"Sy8ra66tWEIk","outputId":"8930ecc4-d1fe-43b3-ed43-887e78037655"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive (required every time)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozxsLUiGWFts"},"outputs":[],"source":["# Define and check the paths\n","# PROJECT_ROOT assumes the shared Milestone II folder is in your root google drive\n","PROJECT_ROOT = '/content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues' # Nathan's Drive\n","DATA_DIR = f\"{PROJECT_ROOT}/data\"\n","NOTEBOOK_DIR = f\"{PROJECT_ROOT}/notebooks\"\n","OUTPUT_DIR = f\"{PROJECT_ROOT}/outputs\"\n","IND_DIR = f'{DATA_DIR}/verified_indicators.csv'\n","\n","if not os.path.exists(PROJECT_ROOT):\n","    PROJECT_ROOT = os.path.abspath(\"..\")  # fallback for local runs"]},{"cell_type":"markdown","metadata":{"id":"LcqqS20dFSQa"},"source":["What I'd Actually Recommend ðŸŽ¯\n","For your 15k wordplay terms:\n","\n","- Start with this analysis to get a ballpark (15-40 clusters seems reasonable)\n","- Use DBSCAN instead of k-means - it can find variable-density clusters and label outliers/noise\n","- Try hierarchical clustering with a dendrogram - this is probably better for wordplay since categories naturally nest (e.g., \"phonetic wordplay\" â†’ \"rhymes\" â†’ \"perfect rhymes\" vs \"slant rhymes\")\n","- Sample and validate - Pick 5-10 clusters, look at 20 random words from each, ask \"do these belong together?\"\n","- Consider manual seeding - If you have examples of clear categories, use semi-supervised clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1770520810905,"user":{"displayName":"Nathan Cantwell","userId":"15236319868573600434"},"user_tz":480},"id":"cP7WhB21VtdP","outputId":"5e3777d8-eea8-4ec6-8e4a-2ce863894922"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import string\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from sentence_transformers import SentenceTransformer\n","from sklearn.preprocessing import normalize\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score\n","import seaborn as sns\n","from sklearn.cluster import KMeans, DBSCAN, HDBSCAN\n","from sklearn.manifold import TSNE\n","import umap\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","\n","\"\"\"\n","Embedding Model Comparison for Wordplay Clustering\n","Compares multiple embedding models and creates visualizations\n","\"\"\"\n","\n","class EmbeddingComparison:\n","    \"\"\"Compare different embedding models for clustering wordplay terms\"\"\"\n","\n","    def __init__(self, words, output_dir=OUTPUT_DIR):\n","        self.words = words\n","        self.output_dir = output_dir\n","        self.results = {}\n","        os.makedirs(self.output_dir, exist_ok=True)\n","\n","    def get_sentence_transformer_embeddings(self, model_name):\n","        \"\"\"Get embeddings using sentence-transformers\"\"\"\n","        print(f\"Loading {model_name}...\")\n","        model = SentenceTransformer(model_name)\n","        print(f\"Generating embeddings for {len(self.words)} words...\")\n","        embeddings = model.encode(self.words, show_progress_bar=True, batch_size=32)\n","        embeddings = normalize(embeddings)\n","        return embeddings\n","\n","    def reduce_dimensions(self, embeddings, n_components=100):\n","        print(f\"ðŸ”¹ Applying PCA for clustering: {n_components} dims\")\n","\n","        pca = PCA(n_components=n_components, random_state=42)\n","        reduced = pca.fit_transform(embeddings)\n","\n","        # re-normalize after PCA\n","        reduced = normalize(reduced)\n","\n","        explained = np.sum(pca.explained_variance_ratio_)\n","        print(f\"   â†’ Variance retained: {explained:.2%}\")\n","\n","        return reduced\n","\n","    def cluster_embeddings(self, embeddings, n_clusters=10, method='kmeans'):\n","        \"\"\"Cluster embeddings using specified method\"\"\"\n","        if method == 'kmeans':\n","            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n","            labels = clusterer.fit_predict(embeddings)\n","        elif method == 'dbscan':\n","            clusterer = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n","            labels = clusterer.fit_predict(embeddings)\n","        elif method == 'hierarchical':\n","            from scipy.cluster.hierarchy import linkage, fcluster\n","            from scipy.spatial.distance import squareform\n","            from sklearn.metrics.pairwise import cosine_distances\n","\n","            # Compute distance matrix\n","            distances = cosine_distances(embeddings)\n","            condensed_distances = squareform(distances)\n","\n","            # Perform hierarchical clustering\n","            Z = linkage(condensed_distances, method='average')\n","\n","            # Cut tree to get n_clusters\n","            labels = fcluster(Z, n_clusters, criterion='maxclust') - 1  # -1 to make 0-indexed\n","        else:\n","            raise ValueError(f\"Unknown clustering method: {method}\")\n","\n","        return labels\n","\n","    def _compute_centroids(self, embeddings, labels):\n","        \"\"\"Helper to compute centroids of non-noise clusters.\"\"\"\n","        centroids = []\n","        cluster_ids = sorted([l for l in set(labels) if l != -1])\n","\n","        if not cluster_ids:  # Handle case where no non-noise clusters exist\n","            return None, None  # Return None for centroids and cluster_ids\n","\n","        for lab in cluster_ids:\n","            centroids.append(embeddings[labels == lab].mean(axis=0))\n","\n","        return np.vstack(centroids), cluster_ids\n","\n","\n","    def auto_dbscan(self, embeddings, min_samples=5, eps_range=None):\n","        \"\"\"\n","        Automatically find good DBSCAN parameters using silhouette score\n","        NC 2/7/26, added compute_centroids and noise points reassignment\n","\n","        Args:\n","            embeddings: Embedding vectors\n","            min_samples: Minimum samples per cluster\n","            eps_range: Range of eps values to try (default: np.arange(0.1, 1.0, 0.1))\n","\n","        Returns:\n","            Best labels, best eps value, and metrics\n","        \"\"\"\n","        if eps_range is None:\n","            #eps_range = np.arange(0.1, 1.0, 0.1)\n","            eps_range = np.arange(0.1, 0.5, 0.05)\n","\n","        best_score = -1\n","        best_eps = None\n","        best_labels = None\n","        results = []\n","\n","        print(f\"  Testing DBSCAN with eps values: {list(eps_range)}\")\n","\n","        for eps in eps_range:\n","            clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n","            labels = clusterer.fit_predict(embeddings)\n","\n","            # Count clusters (excluding noise points labeled -1)\n","            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","            n_noise = list(labels).count(-1)\n","\n","            # If no actual clusters are found, we cannot compute centroids or meaningful metrics.\n","            if n_clusters == 0:\n","                continue\n","\n","            # Reassign ALL noise labels to closest centroid.\n","            new_labels = labels.copy()\n","            if n_noise > 0: # Only try to reassign if there are noise points\n","                centroids, cluster_ids = self._compute_centroids(embeddings, labels)\n","                for i, x in enumerate(embeddings):\n","                    if labels[i] == -1:   # was noise\n","                        sims = centroids @ x   # cosine similarity\n","                        new_labels[i] = cluster_ids[np.argmax(sims)]\n","\n","            # Need at least 2 clusters for these metrics\n","            if n_clusters < 2:\n","                continue  # Need at least 2 clusters for metrics\n","\n","            # Calculate silhouette for non-noise points\n","            mask = labels != -1\n","            if mask.sum() > 0:\n","                filtered_embeddings = embeddings[mask]\n","                filtered_labels = labels[mask]\n","\n","                if len(set(filtered_labels)) >= 2:\n","                    silhouette = silhouette_score(filtered_embeddings, filtered_labels)\n","\n","                    results.append({\n","                        'eps': eps,\n","                        'n_clusters': n_clusters,\n","                        'n_noise': n_noise,\n","                        'silhouette': silhouette\n","                    })\n","\n","                    if silhouette > best_score:\n","                        best_score = silhouette\n","                        best_eps = eps\n","                        best_labels = new_labels # Store labels with reassigned noise\n","\n","        if best_labels is None:\n","            print(\"  Warning: Could not find good DBSCAN parameters, using default eps=0.5\")\n","            clusterer = DBSCAN(eps=0.5, min_samples=min_samples, metric='cosine')\n","            best_labels = clusterer.fit_predict(embeddings)\n","            best_eps = 0.5\n","\n","        return best_labels, best_eps, results\n","\n","    def evaluate_clustering(self, embeddings, labels):\n","        \"\"\"Evaluate clustering quality\"\"\"\n","        # Filter out noise points (label = -1) for DBSCAN\n","        mask = labels != -1\n","        if not mask.any():\n","            return {'silhouette': None, 'davies_bouldin': None, 'n_clusters': 0}\n","\n","        filtered_embeddings = embeddings[mask]\n","        filtered_labels = labels[mask]\n","\n","        # Need at least 2 clusters for these metrics\n","        n_clusters = len(np.unique(filtered_labels))\n","        if n_clusters < 2:\n","            return {'silhouette': None, 'davies_bouldin': None, 'n_clusters': n_clusters}\n","\n","        silhouette = silhouette_score(filtered_embeddings, filtered_labels)\n","        davies_bouldin = davies_bouldin_score(filtered_embeddings, filtered_labels)\n","\n","        return {\n","            'silhouette': silhouette,\n","            'davies_bouldin': davies_bouldin,\n","            'n_clusters': n_clusters\n","        }\n","\n","    def create_visualization(self, reduced_embeddings, labels, model_name,\n","                           reduction_method, clustering_method, metrics):\n","        \"\"\"Create and save visualization\"\"\"\n","        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n","\n","        # Plot 1: Colored by cluster\n","        scatter1 = axes[0].scatter(\n","            reduced_embeddings[:, 0],\n","            reduced_embeddings[:, 1],\n","            c=labels,\n","            cmap='tab20',\n","            alpha=0.6,\n","            s=10\n","        )\n","        axes[0].set_title(\n","            f'{model_name}\\n{reduction_method.upper()} + {clustering_method}',\n","            fontsize=14,\n","            fontweight='bold'\n","        )\n","        axes[0].set_xlabel(f'{reduction_method.upper()} 1')\n","        axes[0].set_ylabel(f'{reduction_method.upper()} 2')\n","        plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n","\n","        # Plot 2: Density plot\n","        axes[1].hexbin(\n","            reduced_embeddings[:, 0],\n","            reduced_embeddings[:, 1],\n","            gridsize=50,\n","            cmap='YlOrRd',\n","            mincnt=1\n","        )\n","        axes[1].set_title(\n","            f'{model_name}\\nDensity Visualization',\n","            fontsize=14,\n","            fontweight='bold'\n","        )\n","        axes[1].set_xlabel(f'{reduction_method.upper()} 1')\n","        axes[1].set_ylabel(f'{reduction_method.upper()} 2')\n","\n","        # Add metrics to plot\n","        metrics_text = f\"Clusters: {metrics['n_clusters']}\\n\"\n","        if metrics['silhouette'] is not None:\n","            metrics_text += f\"Silhouette: {metrics['silhouette']:.3f}\\n\"\n","            metrics_text += f\"Davies-Bouldin: {metrics['davies_bouldin']:.3f}\"\n","        else:\n","            metrics_text += \"Metrics: N/A\"\n","\n","        fig.text(0.5, 0.02, metrics_text, ha='center', fontsize=12,\n","                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","        plt.tight_layout()\n","        plt.subplots_adjust(bottom=0.1)\n","\n","        # Save figure\n","        safe_name = model_name.replace('/', '_').replace(' ', '_')\n","        filename = f'{safe_name}_{reduction_method}_{clustering_method}.png'\n","        filepath = f'{self.output_dir}/{filename}'\n","        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        return filepath\n","\n","    def analyze_model(self, model_name, embeddings=None,\n","                     #reduction_methods=['umap', 'tsne'],\n","                     reduction_methods=['PCA'],\n","                     clustering_methods=['kmeans'],\n","                     n_clusters=10):\n","        \"\"\"Complete analysis pipeline for one model\"\"\"\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Analyzing: {model_name}\")\n","        print(f\"{'='*60}\")\n","\n","        if embeddings is None:\n","            embeddings = self.get_sentence_transformer_embeddings(model_name)\n","\n","        model_results = {\n","            'embeddings': embeddings,\n","            'visualizations': []\n","        }\n","\n","        for reduction in reduction_methods:\n","            reduced = self.reduce_dimensions(embeddings)\n","\n","            for clustering in clustering_methods:\n","                labels = self.cluster_embeddings(\n","                    embeddings,\n","                    n_clusters=n_clusters,\n","                    method=clustering\n","                )\n","\n","                metrics = self.evaluate_clustering(embeddings, labels)\n","\n","                viz_path = self.create_visualization(\n","                    reduced, labels, model_name,\n","                    reduction, clustering, metrics\n","                )\n","\n","                model_results['visualizations'].append({\n","                    'reduction': reduction,\n","                    'clustering': clustering,\n","                    'metrics': metrics,\n","                    'path': viz_path\n","                })\n","\n","                print(f\"  {reduction} + {clustering}: {metrics}\")\n","\n","        self.results[model_name] = model_results\n","        return model_results\n","\n","    def compare_cluster_counts(self, model_names, cluster_range, reduction_method='umap'):\n","        \"\"\"\n","        Compare models across different cluster counts\n","\n","        Args:\n","            model_names: List of model names to compare\n","            cluster_range: List or range of cluster counts to try\n","            reduction_method: Which dimensionality reduction to use ('umap' or 'tsne')\n","\n","        Returns:\n","            DataFrame with metrics for each model and cluster count\n","        \"\"\"\n","        results_data = []\n","\n","        for model_name in model_names:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"Testing {model_name} with varying cluster counts\")\n","            print(f\"{'='*60}\")\n","\n","            # Get or generate embeddings for this model\n","            if model_name in self.results and 'embeddings' in self.results[model_name]:\n","                embeddings = self.results[model_name]['embeddings']\n","                print(f\"Using cached embeddings for {model_name}\")\n","            else:\n","                embeddings = self.get_sentence_transformer_embeddings(model_name)\n","                if model_name not in self.results:\n","                    self.results[model_name] = {}\n","                self.results[model_name]['embeddings'] = embeddings\n","\n","            # Test each cluster count\n","            for n_clusters in cluster_range:\n","                print(f\"  Testing with {n_clusters} clusters...\")\n","\n","                labels = self.cluster_embeddings(\n","                    embeddings,\n","                    n_clusters=n_clusters,\n","                    method='kmeans'\n","                )\n","\n","                metrics = self.evaluate_clustering(embeddings, labels)\n","\n","                results_data.append({\n","                    'Model': model_name,\n","                    'N_Clusters': n_clusters,\n","                    'Silhouette': metrics['silhouette'],\n","                    'Davies_Bouldin': metrics['davies_bouldin'],\n","                    'Actual_Clusters': metrics['n_clusters']\n","                })\n","\n","        df = pd.DataFrame(results_data)\n","\n","        # Save to CSV\n","        csv_path = f'{self.output_dir}/cluster_count_comparison.csv'\n","        df.to_csv(csv_path, index=False)\n","\n","        # Create visualization\n","        viz_path = self.visualize_cluster_comparison(df)\n","\n","        return df, csv_path, viz_path\n","\n","    def find_and_export_optimal_clusters(self, model_names=None, cluster_range=None,\n","                                        methods=['kmeans', 'dbscan', 'hierarchical']):\n","        \"\"\"\n","        Find optimal cluster configuration for each model and export members\n","\n","        Args:\n","            model_names: List of models to analyze (uses all cached if None)\n","            cluster_range: Range of cluster counts to test for kmeans/hierarchical\n","            methods: Clustering methods to try\n","\n","        Returns:\n","            Dictionary with results for each model and method\n","        \"\"\"\n","        if model_names is None:\n","            model_names = list(self.results.keys())\n","\n","        if cluster_range is None:\n","            cluster_range = range(5, 31, 5)\n","\n","        optimal_results = {}\n","\n","        for model_name in model_names:\n","            print(f\"\\n{'='*70}\")\n","            print(f\"Finding optimal clusters for {model_name}\")\n","            print(f\"{'='*70}\")\n","\n","            # Get embeddings\n","            if model_name in self.results and 'embeddings' in self.results[model_name]:\n","                embeddings = self.results[model_name]['embeddings']\n","            else:\n","                print(f\"  Generating embeddings...\")\n","                embeddings = self.get_sentence_transformer_embeddings(model_name)\n","                if model_name not in self.results:\n","                    self.results[model_name] = {}\n","                self.results[model_name]['embeddings'] = embeddings\n","\n","            model_results = {}\n","\n","            # Test K-means\n","            if 'kmeans' in methods:\n","                print(f\"\\n  Testing K-means across {len(cluster_range)} cluster counts...\")\n","                best_score = -1\n","                best_n = None\n","                best_labels = None\n","\n","                for n in cluster_range:\n","                    labels = self.cluster_embeddings(embeddings, n_clusters=n, method='kmeans')\n","                    metrics = self.evaluate_clustering(embeddings, labels)\n","\n","                    if metrics['silhouette'] and metrics['silhouette'] > best_score:\n","                        best_score = metrics['silhouette']\n","                        best_n = n\n","                        best_labels = labels\n","\n","                print(f\"  âœ“ K-means optimal: {best_n} clusters (Silhouette: {best_score:.4f})\")\n","\n","                # Export members\n","                csv_path, txt_path = self.export_cluster_members(\n","                    f\"{model_name}_kmeans_{best_n}\",\n","                    best_labels\n","                )\n","\n","                model_results['kmeans'] = {\n","                    'n_clusters': best_n,\n","                    'silhouette': best_score,\n","                    'labels': best_labels,\n","                    'csv_path': csv_path,\n","                    'txt_path': txt_path\n","                }\n","\n","            # Test DBSCAN\n","            if 'dbscan' in methods:\n","                print(f\"\\n  Testing DBSCAN with auto-tuned parameters...\")\n","                best_labels, best_eps, dbscan_results = self.auto_dbscan(embeddings)\n","\n","                n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n","                n_noise = list(best_labels).count(-1)\n","\n","                metrics = self.evaluate_clustering(embeddings, best_labels)\n","\n","                print(f\"  âœ“ DBSCAN optimal: eps={best_eps:.2f}, {n_clusters} clusters, \"\n","                      f\"{n_noise} noise points (Silhouette: {metrics['silhouette']:.4f})\")\n","\n","                # Export members\n","                csv_path, txt_path = self.export_cluster_members(\n","                    f\"{model_name}_dbscan_eps{best_eps:.2f}\",\n","                    best_labels\n","                )\n","\n","                model_results['dbscan'] = {\n","                    'eps': best_eps,\n","                    'n_clusters': n_clusters,\n","                    'n_noise': n_noise,\n","                    'silhouette': metrics['silhouette'],\n","                    'labels': best_labels,\n","                    'csv_path': csv_path,\n","                    'txt_path': txt_path,\n","                    'tuning_results': dbscan_results\n","                }\n","\n","            # Test Hierarchical\n","            if 'hierarchical' in methods:\n","                print(f\"\\n  Testing Hierarchical clustering...\")\n","\n","                # Create dendrogram\n","                dendro_path = self.create_dendrogram(embeddings, model_name)\n","\n","                # Find best cut\n","                best_score = -1\n","                best_n = None\n","                best_labels = None\n","\n","                for n in cluster_range:\n","                    labels = self.cluster_embeddings(embeddings, n_clusters=n, method='hierarchical')\n","                    metrics = self.evaluate_clustering(embeddings, labels)\n","\n","                    if metrics['silhouette'] and metrics['silhouette'] > best_score:\n","                        best_score = metrics['silhouette']\n","                        best_n = n\n","                        best_labels = labels\n","\n","                print(f\"  âœ“ Hierarchical optimal: {best_n} clusters (Silhouette: {best_score:.4f})\")\n","\n","                # Export members\n","                csv_path, txt_path = self.export_cluster_members(\n","                    f\"{model_name}_hierarchical_{best_n}\",\n","                    best_labels\n","                )\n","\n","                model_results['hierarchical'] = {\n","                    'n_clusters': best_n,\n","                    'silhouette': best_score,\n","                    'labels': best_labels,\n","                    'csv_path': csv_path,\n","                    'txt_path': txt_path,\n","                    'dendrogram_path': dendro_path\n","                }\n","\n","            optimal_results[model_name] = model_results\n","\n","            # Test HDBSCAN\n","            if 'hdbscan' in methods:\n","                print(f\"\\n  Testing HDBSCAN clustering...\")\n","\n","                def noise_aware_score(X, labels, alpha=1.0, beta=0.8):\n","                    # Use to penalize HDBSCAN models with high % of noise points.\n","                    # Increase beta to increase penalization.\n","                    # fraction of noise points\n","                    noise_pct = np.mean(labels == -1)\n","\n","                    # compute silhouette only on non-noise points\n","                    mask = labels != -1\n","                    if len(set(labels[mask])) > 1:\n","                        sil = silhouette_score(X[mask], labels[mask])\n","                    else:\n","                        sil = -1\n","\n","                    # combined objective\n","                    score = alpha * sil - beta * noise_pct\n","                    return score, sil, noise_pct\n","\n","                best_score = -1\n","                best_params = None\n","                best_labels = None\n","\n","                # Grid search over min_cluster_size and min_samples\n","                for min_cluster_size in [10, 15, 20]:\n","                    for min_samples in [1, 5, 10]:\n","                        print(f\"Min size:{min_cluster_size}, min samples:{min_samples}\")\n","\n","                        clusterer = HDBSCAN(\n","                            min_cluster_size=min_cluster_size,\n","                            min_samples=min_samples,\n","                            metric='euclidean'\n","                        )\n","                        labels = clusterer.fit_predict(embeddings)\n","\n","                        # Skip if all points are labeled as noise\n","                        if np.all(labels == -1):\n","                            continue\n","\n","                        score, sil, noise_pct = noise_aware_score(embeddings, labels)\n","\n","                        if score > best_score:\n","                            best_score = score\n","                            best_params = (min_cluster_size, min_samples)\n","                            best_labels = labels\n","                            best_sil = sil\n","                            best_noise = noise_pct\n","\n","                if best_labels is None:\n","                    print(\"  âš ï¸ HDBSCAN failed to produce any clusters.\")\n","                else:\n","                    hdbscan_n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n","                    hdbscan_n_noise = list(best_labels).count(-1)\n","                    print(f\"  âœ“ HDBSCAN optimal: min_cluster_size={best_params[0]}, \"\n","                          f\"min_samples={best_params[1]} (Silhouette: {best_score:.4f})\")\n","\n","                    # Export members\n","                    csv_path, txt_path = self.export_cluster_members(\n","                        f\"{model_name}_hdbscan_{best_params[0]}_{best_params[1]}\",\n","                        best_labels\n","                    )\n","\n","                    model_results['hdbscan'] = {\n","                        'min_cluster_size': best_params[0],\n","                        'min_samples': best_params[1],\n","                        'n_clusters': hdbscan_n_clusters,\n","                        'n_noise': hdbscan_n_noise,\n","                        'silhouette': best_score,\n","                        'labels': best_labels,\n","                        'csv_path': csv_path,\n","                        'txt_path': txt_path\n","                    }\n","\n","            optimal_results[model_name] = model_results\n","\n","        # Create summary report\n","        summary_path = self._create_optimal_clusters_summary(optimal_results)\n","\n","        print(f\"\\n{'='*70}\")\n","        print(\"OPTIMAL CLUSTER ANALYSIS COMPLETE!\")\n","        print(f\"{'='*70}\")\n","        print(f\"\\nSummary report: {summary_path}\")\n","\n","        return optimal_results\n","\n","    def _create_optimal_clusters_summary(self, optimal_results):\n","        \"\"\"Create a summary report of optimal clustering results\"\"\"\n","        summary_path = f'{self.output_dir}/optimal_clusters_summary.txt'\n","\n","        with open(summary_path, 'w', encoding='utf-8') as f:\n","            f.write(\"OPTIMAL CLUSTERING ANALYSIS SUMMARY\\n\")\n","            f.write(\"=\" * 70 + \"\\n\\n\")\n","\n","            for model_name, methods in optimal_results.items():\n","                f.write(f\"MODEL: {model_name}\\n\")\n","                f.write(\"-\" * 70 + \"\\n\\n\")\n","\n","                for method, results in methods.items():\n","                    f.write(f\"  {method.upper()}:\\n\")\n","\n","                    if method == 'dbscan' or method == 'hdbscan': # Modified here\n","                        if method == 'hdbscan':\n","                            f.write(f\"    min_cluster_size: {results['min_cluster_size']}\\n\")\n","                            f.write(f\"    min_samples: {results['min_samples']}\\n\")\n","                        else: # dbscan\n","                            f.write(f\"    eps: {results['eps']:.2f}\\n\")\n","                        f.write(f\"    Clusters: {results['n_clusters']}\\n\")\n","                        f.write(f\"    Noise points: {results['n_noise']}\\n\")\n","                    else:\n","                        f.write(f\"    Optimal clusters: {results['n_clusters']}\\n\")\n","\n","                    f.write(f\"    Silhouette score: {results['silhouette']:.4f}\\n\")\n","                    f.write(f\"    Full data: {results['csv_path'].split('/')[-1]}\\n\")\n","                    f.write(f\"    Summary: {results['txt_path'].split('/')[-1]}\\n\")\n","\n","                    if 'dendrogram_path' in results:\n","                        f.write(f\"    Dendrogram: {results['dendrogram_path'].split('/')[-1]}\\n\")\n","\n","                    f.write(\"\\n\")\n","\n","                f.write(\"\\n\")\n","\n","            # Recommendations\n","            f.write(\"=\" * 70 + \"\\n\")\n","            f.write(\"RECOMMENDATIONS\\n\")\n","            f.write(\"=\" * 70 + \"\\n\\n\")\n","\n","            f.write(\"For each model, the best clustering method is shown:\\n\\n\")\n","\n","            for model_name, methods in optimal_results.items():\n","                best_method = None\n","                best_score = -1\n","\n","                for method, results in methods.items():\n","                    if results['silhouette'] > best_score:\n","                        best_score = results['silhouette']\n","                        best_method = method\n","\n","                f.write(f\"{model_name}:\\n\")\n","                f.write(f\"  â†’ {best_method.upper()} (Silhouette: {best_score:.4f})\\n\")\n","\n","                if best_method == 'dbscan' or best_method == 'hdbscan': # Modified here\n","                    f.write(f\"    {methods[best_method]['n_clusters']} clusters, \"\n","                           f\"{methods[best_method]['n_noise']} noise points\\n\")\n","                else:\n","                    f.write(f\"    {methods[best_method]['n_clusters']} clusters\\n\")\n","\n","                f.write(f\"    Results: {methods[best_method]['txt_path'].split('/')[-1]}\\n\\n\")\n","\n","        return summary_path\n","\n","    def compare_methods_across_clusters(self, model_name, cluster_range,\n","                                       methods=['kmeans', 'hierarchical']):\n","        \"\"\"\n","        Compare different clustering methods across cluster counts\n","\n","        Args:\n","            model_name: Embedding model to use\n","            cluster_range: Range of cluster counts to test\n","            methods: List of methods to compare (dbscan excluded as it doesn't use n_clusters)\n","\n","        Returns:\n","            DataFrame with results and path to visualization\n","        \"\"\"\n","        print(f\"\\n{'='*70}\")\n","        print(f\"Comparing clustering methods for {model_name}\")\n","        print(f\"{'='*70}\")\n","\n","        # Get embeddings\n","        if model_name in self.results and 'embeddings' in self.results[model_name]:\n","            embeddings = self.results[model_name]['embeddings']\n","        else:\n","            print(f\"Generating embeddings...\")\n","            embeddings = self.get_sentence_transformer_embeddings(model_name)\n","            if model_name not in self.results:\n","                self.results[model_name] = {}\n","            self.results[model_name]['embeddings'] = embeddings\n","\n","        results_data = []\n","\n","        # Test each method\n","        for method in methods:\n","            print(f\"\\nTesting {method.upper()}...\")\n","\n","            for n_clusters in cluster_range:\n","                print(f\"  {n_clusters} clusters...\", end=' ')\n","\n","                try:\n","                    labels = self.cluster_embeddings(\n","                        embeddings,\n","                        n_clusters=n_clusters,\n","                        method=method\n","                    )\n","\n","                    metrics = self.evaluate_clustering(embeddings, labels)\n","\n","                    results_data.append({\n","                        'Method': method,\n","                        'N_Clusters': n_clusters,\n","                        'Silhouette': metrics['silhouette'],\n","                        'Davies_Bouldin': metrics['davies_bouldin'],\n","                        'Actual_Clusters': metrics['n_clusters']\n","                    })\n","\n","                    print(f\"Silhouette: {metrics['silhouette']:.4f}\")\n","\n","                except Exception as e:\n","                    print(f\"Error: {e}\")\n","                    continue\n","\n","        # Add DBSCAN results if requested\n","        if 'dbscan' in methods:\n","            print(f\"\\nTesting DBSCAN (auto-tuning eps)...\")\n","            best_labels, best_eps, dbscan_tuning = self.auto_dbscan(embeddings)\n","\n","            metrics = self.evaluate_clustering(embeddings, best_labels)\n","            n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)\n","\n","            results_data.append({\n","                'Method': 'dbscan',\n","                'N_Clusters': n_clusters,\n","                'Silhouette': metrics['silhouette'],\n","                'Davies_Bouldin': metrics['davies_bouldin'],\n","                'Actual_Clusters': n_clusters\n","            })\n","\n","            print(f\"  Best: {n_clusters} clusters (eps={best_eps:.2f}), \"\n","                  f\"Silhouette: {metrics['silhouette']:.4f}\")\n","\n","        df = pd.DataFrame(results_data)\n","\n","        # Save to CSV\n","        safe_name = model_name.replace('/', '_').replace(' ', '_')\n","        csv_path = f'{self.output_dir}/{safe_name}_method_comparison.csv'\n","        df.to_csv(csv_path, index=False)\n","\n","        # Create visualization\n","        viz_path = self._visualize_method_comparison(df, model_name)\n","\n","        return df, csv_path, viz_path\n","\n","    def _visualize_method_comparison(self, df, model_name):\n","        \"\"\"Create visualization comparing clustering methods\"\"\"\n","\n","        # Separate DBSCAN if present (it has only one point)\n","        df_methods = df[df['Method'] != 'dbscan']\n","        df_dbscan = df[df['Method'] == 'dbscan']\n","\n","        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # Color palette\n","        methods = df_methods['Method'].unique()\n","        colors = plt.cm.Set2(np.linspace(0, 1, len(methods)))\n","        color_map = {method: colors[i] for i, method in enumerate(methods)}\n","\n","        # Plot 1: Silhouette Score\n","        for method in methods:\n","            method_data = df_methods[df_methods['Method'] == method]\n","            axes[0].plot(\n","                method_data['N_Clusters'],\n","                method_data['Silhouette'],\n","                marker='o',\n","                linewidth=2.5,\n","                markersize=8,\n","                label=method.upper(),\n","                color=color_map[method]\n","            )\n","\n","        # Add DBSCAN as a single point if present\n","        if not df_dbscan.empty:\n","            axes[0].scatter(\n","                df_dbscan['N_Clusters'],\n","                df_dbscan['Silhouette'],\n","                marker='*',\n","                s=400,\n","                color='red',\n","                label='DBSCAN (auto)',\n","                zorder=5,\n","                edgecolors='black',\n","                linewidth=1.5\n","            )\n","\n","        axes[0].set_xlabel('Number of Clusters', fontsize=12, fontweight='bold')\n","        axes[0].set_ylabel('Silhouette Score', fontsize=12, fontweight='bold')\n","        axes[0].set_title(\n","            f'Silhouette Score by Clustering Method\\n{model_name}',\n","            fontsize=13,\n","            fontweight='bold',\n","            pad=15\n","        )\n","        axes[0].legend(fontsize=10, loc='best', framealpha=0.9)\n","        axes[0].grid(True, alpha=0.3, linestyle='--')\n","        axes[0].axhline(y=0, color='gray', linestyle='-', linewidth=1, alpha=0.5)\n","\n","        # Plot 2: Davies-Bouldin Index\n","        for method in methods:\n","            method_data = df_methods[df_methods['Method'] == method]\n","            axes[1].plot(\n","                method_data['N_Clusters'],\n","                method_data['Davies_Bouldin'],\n","                marker='o',\n","                linewidth=2.5,\n","                markersize=8,\n","                label=method.upper(),\n","                color=color_map[method]\n","            )\n","\n","        # Add DBSCAN as a single point if present\n","        if not df_dbscan.empty:\n","            axes[1].scatter(\n","                df_dbscan['N_Clusters'],\n","                df_dbscan['Davies_Bouldin'],\n","                marker='*',\n","                s=400,\n","                color='red',\n","                label='DBSCAN (auto)',\n","                zorder=5,\n","                edgecolors='black',\n","                linewidth=1.5\n","            )\n","\n","        axes[1].set_xlabel('Number of Clusters', fontsize=12, fontweight='bold')\n","        axes[1].set_ylabel('Davies-Bouldin Index', fontsize=12, fontweight='bold')\n","        axes[1].set_title(\n","            f'Davies-Bouldin Index by Clustering Method\\n{model_name}',\n","            fontsize=13,\n","            fontweight='bold',\n","            pad=15\n","        )\n","        axes[1].legend(fontsize=10, loc='best', framealpha=0.9)\n","        axes[1].grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","\n","        # Save\n","        safe_name = model_name.replace('/', '_').replace(' ', '_')\n","        viz_path = f'{self.output_dir}/{safe_name}_method_comparison.png'\n","        plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        print(f\"\\nVisualization saved: {viz_path}\")\n","\n","        return viz_path\n","\n","    def visualize_cluster_comparison(self, df):\n","        \"\"\"\n","        Create line plots comparing models across cluster counts\n","\n","        Args:\n","            df: DataFrame with columns: Model, N_Clusters, Silhouette, Davies_Bouldin\n","\n","        Returns:\n","            Path to saved visualization\n","        \"\"\"\n","        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","        # Color palette for models\n","        models = df['Model'].unique()\n","        colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n","        color_map = {model: colors[i] for i, model in enumerate(models)}\n","\n","        # Plot 1: Silhouette Score vs Cluster Count\n","        for model in models:\n","            model_data = df[df['Model'] == model]\n","            axes[0].plot(\n","                model_data['N_Clusters'],\n","                model_data['Silhouette'],\n","                marker='o',\n","                linewidth=2,\n","                label=model.split('/')[-1],  # Use short name\n","                color=color_map[model]\n","            )\n","\n","        axes[0].set_xlabel('Number of Clusters', fontsize=11)\n","        axes[0].set_ylabel('Silhouette Score', fontsize=11)\n","        axes[0].set_title('Silhouette Score vs Cluster Count\\n(higher is better)',\n","                         fontsize=12, fontweight='bold')\n","        axes[0].legend(fontsize=9, loc='best')\n","        axes[0].grid(True, alpha=0.3)\n","        axes[0].axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n","\n","        # Plot 2: Davies-Bouldin Index vs Cluster Count\n","        for model in models:\n","            model_data = df[df['Model'] == model]\n","            axes[1].plot(\n","                model_data['N_Clusters'],\n","                model_data['Davies_Bouldin'],\n","                marker='o',\n","                linewidth=2,\n","                label=model.split('/')[-1],\n","                color=color_map[model]\n","            )\n","\n","        axes[1].set_xlabel('Number of Clusters', fontsize=11)\n","        axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)\n","        axes[1].set_title('Davies-Bouldin Index vs Cluster Count\\n(lower is better)',\n","                         fontsize=12, fontweight='bold')\n","        axes[1].legend(fontsize=9, loc='best')\n","        axes[1].grid(True, alpha=0.3)\n","\n","        # Plot 3: Combined Score (normalized)\n","        # Normalize both metrics to 0-1 scale where better = higher\n","        for model in models:\n","            model_data = df[df['Model'] == model].copy()\n","\n","            # Normalize silhouette (already -1 to 1, shift to 0-1)\n","            norm_silhouette = (model_data['Silhouette'] + 1) / 2\n","\n","            # Normalize Davies-Bouldin (invert and scale)\n","            # Lower DB is better, so we invert it\n","            max_db = df['Davies_Bouldin'].max()\n","            min_db = df['Davies_Bouldin'].min()\n","            if max_db > min_db:\n","                norm_db = 1 - (model_data['Davies_Bouldin'] - min_db) / (max_db - min_db)\n","            else:\n","                norm_db = model_data['Davies_Bouldin'] * 0  # All same, set to 0\n","\n","            # Combined score (average of normalized metrics)\n","            combined = (norm_silhouette + norm_db) / 2\n","\n","            axes[2].plot(\n","                model_data['N_Clusters'],\n","                combined,\n","                marker='o',\n","                linewidth=2,\n","                label=model.split('/')[-1],\n","                color=color_map[model]\n","            )\n","\n","        axes[2].set_xlabel('Number of Clusters', fontsize=11)\n","        axes[2].set_ylabel('Combined Score', fontsize=11)\n","        axes[2].set_title('Combined Quality Score\\n(higher is better)',\n","                         fontsize=12, fontweight='bold')\n","        axes[2].legend(fontsize=9, loc='best')\n","        axes[2].grid(True, alpha=0.3)\n","        axes[2].set_ylim([0, 1])\n","\n","        plt.tight_layout()\n","\n","        # Save figure\n","        viz_path = f'{self.output_dir}/cluster_count_comparison.png'\n","        plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        return viz_path\n","\n","    def create_dendrogram(self, embeddings, model_name, max_display=50):\n","        \"\"\"\n","        Create a dendrogram visualization for hierarchical clustering\n","\n","        Args:\n","            embeddings: Embedding vectors\n","            model_name: Name of the model\n","            max_display: Maximum number of leaf nodes to display\n","\n","        Returns:\n","            Path to saved dendrogram\n","        \"\"\"\n","        from scipy.cluster.hierarchy import linkage, dendrogram\n","        from scipy.spatial.distance import squareform\n","        from sklearn.metrics.pairwise import cosine_distances\n","\n","        print(f\"Creating dendrogram for {model_name}...\")\n","\n","        # Compute distance matrix\n","        distances = cosine_distances(embeddings)\n","        condensed_distances = squareform(distances)\n","\n","        # Perform hierarchical clustering\n","        Z = linkage(condensed_distances, method='average')\n","\n","        # Create figure\n","        fig, ax = plt.subplots(figsize=(20, 10))\n","\n","        # Plot dendrogram\n","        dendrogram(\n","            Z,\n","            ax=ax,\n","            truncate_mode='lastp',\n","            p=max_display,\n","            leaf_font_size=10,\n","            show_contracted=True\n","        )\n","\n","        ax.set_title(\n","            f'Hierarchical Clustering Dendrogram - {model_name}\\n'\n","            f'(Showing top {max_display} clusters)',\n","            fontsize=14,\n","            fontweight='bold'\n","        )\n","        ax.set_xlabel('Cluster Index or (Sample Count)', fontsize=12)\n","        ax.set_ylabel('Cosine Distance', fontsize=12)\n","\n","        plt.tight_layout()\n","\n","        # Save figure\n","        safe_name = model_name.replace('/', '_').replace(' ', '_')\n","        filename = f'{safe_name}_dendrogram.png'\n","        filepath = f'{self.output_dir}/{filename}'\n","        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        return filepath\n","\n","    def export_cluster_members(self, model_name, labels, n_samples_per_cluster=20):\n","        \"\"\"\n","        Export cluster members to CSV and text files\n","\n","        Args:\n","            model_name: Name of the model\n","            labels: Cluster labels for each word\n","            n_samples_per_cluster: Number of sample words to show per cluster in summary\n","\n","        Returns:\n","            Paths to generated files\n","        \"\"\"\n","        import pandas as pd\n","\n","        print(f\"Exporting cluster members for {model_name}...\")\n","\n","        # Create dataframe\n","        df = pd.DataFrame({\n","            'word': self.words,\n","            'cluster': labels\n","        })\n","\n","        # Sort by cluster\n","        df = df.sort_values('cluster')\n","\n","        # Save full CSV\n","        safe_name = model_name.replace('/', '_').replace(' ', '_')\n","        csv_path = f'{self.output_dir}/{safe_name}_clusters_full.csv'\n","        df.to_csv(csv_path, index=False)\n","\n","        # Create summary text file\n","        txt_path = f'{self.output_dir}/{safe_name}_clusters_summary.txt'\n","\n","        with open(txt_path, 'w', encoding='utf-8') as f:\n","            f.write(f\"Cluster Summary for {model_name}\\n\")\n","            f.write(\"=\" * 70 + \"\\n\\n\")\n","\n","            unique_clusters = sorted(df['cluster'].unique())\n","\n","            # Handle noise points in DBSCAN\n","            if -1 in unique_clusters:\n","                noise_words = df[df['cluster'] == -1]['word'].tolist()\n","                f.write(f\"NOISE POINTS (not assigned to any cluster): {len(noise_words)} words\\n\")\n","                f.write(f\"Sample: {', '.join(noise_words[:20])}\\n\")\n","                if len(noise_words) > 20:\n","                    f.write(f\"... and {len(noise_words) - 20} more\\n\")\n","                f.write(\"\\n\" + \"=\" * 70 + \"\\n\\n\")\n","                unique_clusters = [c for c in unique_clusters if c != -1]\n","\n","            # Write each cluster\n","            for cluster_id in unique_clusters:\n","                cluster_words = df[df['cluster'] == cluster_id]['word'].tolist()\n","\n","                f.write(f\"CLUSTER {cluster_id}: {len(cluster_words)} words\\n\")\n","                f.write(\"-\" * 70 + \"\\n\")\n","\n","                # Show sample\n","                sample = cluster_words[:n_samples_per_cluster]\n","                f.write(f\"Sample ({len(sample)} of {len(cluster_words)}): \")\n","                f.write(\", \".join(sample))\n","\n","                if len(cluster_words) > n_samples_per_cluster:\n","                    f.write(f\"\\n... and {len(cluster_words) - n_samples_per_cluster} more\")\n","\n","                f.write(\"\\n\\n\")\n","\n","            # Statistics\n","            f.write(\"=\" * 70 + \"\\n\")\n","            f.write(\"STATISTICS\\n\")\n","            f.write(\"=\" * 70 + \"\\n\")\n","            f.write(f\"Total words: {len(self.words)}\\n\")\n","            f.write(f\"Number of clusters: {len(unique_clusters)}\\n\")\n","\n","            if len(unique_clusters) > 0:\n","                cluster_sizes = df[df['cluster'] != -1].groupby('cluster').size()\n","                f.write(f\"Average cluster size: {cluster_sizes.mean():.1f}\\n\")\n","                f.write(f\"Largest cluster: {cluster_sizes.max()} words\\n\")\n","                f.write(f\"Smallest cluster: {cluster_sizes.min()} words\\n\")\n","\n","        print(f\"  Saved full data: {csv_path}\")\n","        print(f\"  Saved summary: {txt_path}\")\n","\n","        return csv_path, txt_path\n","\n","    def create_comparison_report(self):\n","        \"\"\"Create a summary report comparing all models\"\"\"\n","        # Collect metrics\n","        comparison_data = []\n","\n","        for model_name, results in self.results.items():\n","            for viz in results['visualizations']:\n","                comparison_data.append({\n","                    'Model': model_name,\n","                    'Reduction': viz['reduction'],\n","                    'Clustering': viz['clustering'],\n","                    'N Clusters': viz['metrics']['n_clusters'],\n","                    'Silhouette': viz['metrics']['silhouette'],\n","                    'Davies-Bouldin': viz['metrics']['davies_bouldin']\n","                })\n","\n","        df = pd.DataFrame(comparison_data)\n","\n","        # Create comparison visualization\n","        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # Silhouette scores\n","        if df['Silhouette'].notna().any():\n","            silhouette_data = df.dropna(subset=['Silhouette'])\n","            silhouette_pivot = silhouette_data.pivot_table(\n","                values='Silhouette',\n","                index='Model',\n","                columns='Reduction',\n","                aggfunc='mean'\n","            )\n","            sns.heatmap(silhouette_pivot, annot=True, fmt='.3f',\n","                       cmap='RdYlGn', ax=axes[0], vmin=-1, vmax=1)\n","            axes[0].set_title('Silhouette Scores (higher is better)', fontweight='bold')\n","\n","        # Davies-Bouldin scores\n","        if df['Davies-Bouldin'].notna().any():\n","            db_data = df.dropna(subset=['Davies-Bouldin'])\n","            db_pivot = db_data.pivot_table(\n","                values='Davies-Bouldin',\n","                index='Model',\n","                columns='Reduction',\n","                aggfunc='mean'\n","            )\n","            sns.heatmap(db_pivot, annot=True, fmt='.3f',\n","                       cmap='RdYlGn_r', ax=axes[1])\n","            axes[1].set_title('Davies-Bouldin Scores (lower is better)', fontweight='bold')\n","\n","        plt.tight_layout()\n","        comparison_path = f'{self.output_dir}/model_comparison.png'\n","        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        # Save metrics table\n","        csv_path = f'{self.output_dir}/comparison_metrics.csv'\n","        df.to_csv(csv_path, index=False)\n","\n","        return comparison_path, csv_path, df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":778,"referenced_widgets":["ca8aa85c5c0b4210be38aa75f3d6df66","62bf95cf11b3402db87bfc842caef361","54c5c3ab55084506b682a33fe72ea7a6","b08fc06b681642d3b45537d627932ea4","f464257a85c64ad5bfe8ca7059aa63c4","829ac1e784114ef5b2227f541647b6b3","a3ea798ad8e241bea9740a4384f8e401","82273124bc4b4058b55bfaaf040626b3","7b302b308ff742259faffe35e1d2a6d9","53c21d1303ce466ba6d9e2c91ccb42f1","a043bfe08440462d951f3534bc7910b1","f535213759bd4f5e80863b21cdcf63d6","96e1f485da69478db85d0f162780bb3c","01b37d4cf22946479ee77285abe7b1f4","7cdedcc52d3b47009648c6e6275552f0","cc0e9b541ea9452dad5424027abff7a3","2717d2ccc57a43efb1b9ef26faeaaa78","40ffa2585053449ebde664705893720e","5c5ece625a0245b89505f2490d802767","ef81235787914e28a22f7f717fe37996","443a07893fef4d51b79310e43b01e9b9","7e217ead9ee34049975b5a02ef6ac30c"]},"id":"yfkkMgWKG-HK","outputId":"5ee68531-fe02-446a-8b52-dc21caa47a01"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","COMPREHENSIVE OPTIMAL CLUSTERING ANALYSIS\n","======================================================================\n","\n","Words: 14195\n","Models: all-mpnet-base-v2\n","Methods: hdbscan\n","Cluster range: [10, 15, 20, 25, 30, 35, 40, 45, 50]\n","\n","This will:\n","  1. Test each model with multiple clustering methods\n","  2. Find optimal parameters for each method\n","  3. Export cluster members to CSV and text files\n","  4. Create dendrograms for hierarchical clustering\n","  5. Generate comparison visualizations\n","\n","======================================================================\n","Finding optimal clusters for all-mpnet-base-v2\n","======================================================================\n","  Generating embeddings...\n","Loading all-mpnet-base-v2...\n"]},{"name":"stderr","output_type":"stream","text":["Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca8aa85c5c0b4210be38aa75f3d6df66","version_major":2,"version_minor":0},"text/plain":["Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n","Key                     | Status     |  | \n","------------------------+------------+--+-\n","embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"name":"stdout","output_type":"stream","text":["Generating embeddings for 14195 words...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f535213759bd4f5e80863b21cdcf63d6","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/444 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","  Testing HDBSCAN clustering...\n","Min size:10, min samples:1\n","Min size:10, min samples:5\n","Min size:10, min samples:10\n","Min size:15, min samples:1\n","Min size:15, min samples:5\n","Min size:15, min samples:10\n","Min size:20, min samples:1\n","Min size:20, min samples:5\n","Min size:20, min samples:10\n","  âœ“ HDBSCAN optimal: min_cluster_size=15, min_samples=10 (Silhouette: -0.2345)\n","Exporting cluster members for all-mpnet-base-v2_hdbscan_15_10...\n","  Saved full data: /content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues/outputs/all-mpnet-base-v2_hdbscan_15_10_clusters_full.csv\n","  Saved summary: /content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues/outputs/all-mpnet-base-v2_hdbscan_15_10_clusters_summary.txt\n","\n","======================================================================\n","OPTIMAL CLUSTER ANALYSIS COMPLETE!\n","======================================================================\n","\n","Summary report: /content/drive/MyDrive/SIADS 692 Milestone II/Milestone II - NLP Cryptic Crossword Clues/outputs/optimal_clusters_summary.txt\n","\n","======================================================================\n","ANALYSIS COMPLETE! Generated files:\n","======================================================================\n","\n","all-mpnet-base-v2:\n","  HDBSCAN:\n","    - all-mpnet-base-v2_hdbscan_15_10_clusters_summary.txt (summary)\n","    - all-mpnet-base-v2_hdbscan_15_10_clusters_full.csv (full data)\n","\n","  Summary: optimal_clusters_summary.txt\n","======================================================================\n","EMBEDDING MODEL COMPARISON FOR WORDPLAY CLUSTERING\n","======================================================================\n","\n","To use this script:\n","2. Uncomment and modify one of the example usage options above\n","3. Run the script\n","======================================================================\n","CPU times: user 1h 5min 21s, sys: 9.42 s, total: 1h 5min 30s\n","Wall time: 1h 5min 54s\n"]}],"source":["%%time\n","\"\"\"\n","Quick Start Script for Embedding Comparison\n","Load your wordplay terms and run the comparison\n","\"\"\"\n","\n","def load_words_from_file(filepath):\n","    \"\"\"\n","    Load words from various file formats\n","    Supports: txt (one word per line), csv, excel\n","    \"\"\"\n","    if filepath.endswith('.txt'):\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            words = [line.strip() for line in f if line.strip()]\n","    elif filepath.endswith('.csv'):\n","        df = pd.read_csv(filepath)\n","        # Assumes words are in first column, adjust if needed\n","        words = df.iloc[:, 0].dropna().astype(str).tolist()\n","    elif filepath.endswith(('.xlsx', '.xls')):\n","        df = pd.read_excel(filepath)\n","        words = df.iloc[:, 0].dropna().astype(str).tolist()\n","    else:\n","        raise ValueError(\"Unsupported file format. Use .txt, .csv, or .xlsx\")\n","\n","    # NC, drop duplicate indicators, lose signal on frequent indicators\n","    # words = list(set(words))\n","    return words\n","\n","def run_comparison(words, n_clusters=10, models=None):\n","    \"\"\"\n","    Run the full comparison pipeline\n","\n","    Args:\n","        words: List of wordplay terms\n","        n_clusters: Number of clusters for k-means (adjust based on your data)\n","        models: List of model names to compare (uses defaults if None)\n","    \"\"\"\n","\n","    # Default models (fast to very good quality)\n","    if models is None:\n","        models = [\n","            'all-MiniLM-L6-v2',          # 384 dim, very fast\n","            'all-mpnet-base-v2',         # 768 dim, high quality\n","            'all-MiniLM-L12-v2',         # 384 dim, balanced\n","        ]\n","\n","    print(f\"\\nAnalyzing {len(words)} unique wordplay terms\")\n","    print(f\"Using {len(models)} embedding models\")\n","    print(f\"Target clusters: {n_clusters}\\n\")\n","\n","    # Initialize\n","    comparator = EmbeddingComparison(words)\n","\n","    # Analyze each model\n","    for i, model in enumerate(models, 1):\n","        print(f\"\\n[{i}/{len(models)}] Processing {model}...\")\n","        try:\n","            comparator.analyze_model(\n","                model,\n","                reduction_methods=['umap', 'tsne'],\n","                clustering_methods=['kmeans'],\n","                n_clusters=n_clusters\n","            )\n","        except Exception as e:\n","            print(f\"Error with {model}: {e}\")\n","            continue\n","\n","    # Create comparison report\n","    if comparator.results:\n","        print(\"\\nGenerating comparison report...\")\n","        comp_path, csv_path, metrics_df = comparator.create_comparison_report()\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"ANALYSIS COMPLETE!\")\n","        print(\"=\"*70)\n","        print(f\"\\nResults saved to: {comparator.output_dir}/\")\n","        print(\"\\nFiles generated:\")\n","        for model in comparator.results.keys():\n","            print(f\"\\n  {model}:\")\n","            for viz in comparator.results[model]['visualizations']:\n","                print(f\"    - {viz['path'].split('/')[-1]}\")\n","        print(f\"\\n  Comparison:\")\n","        print(f\"    - {comp_path.split('/')[-1]}\")\n","        print(f\"    - {csv_path.split('/')[-1]}\")\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"Metrics Summary:\")\n","        print(\"=\"*70)\n","        print(metrics_df.to_string())\n","\n","        return comparator\n","    else:\n","        print(\"\\nNo results generated. Check for errors above.\")\n","        return None\n","\n","\n","def run_cluster_count_comparison(words, cluster_range=None, models=None):\n","    \"\"\"\n","    Compare models across different cluster counts\n","\n","    Args:\n","        words: List of wordplay terms\n","        cluster_range: List or range of cluster counts to test (default: 5 to 30 by 5)\n","        models: List of model names to compare (uses defaults if None)\n","\n","    Returns:\n","        EmbeddingComparison object with results\n","    \"\"\"\n","\n","    # Default models\n","    if models is None:\n","        models = [\n","            'all-MiniLM-L6-v2',\n","            'all-mpnet-base-v2',\n","            'all-MiniLM-L12-v2',\n","        ]\n","\n","    # Default cluster range\n","    if cluster_range is None:\n","        cluster_range = range(5, 31, 5)  # 5, 10, 15, 20, 25, 30\n","\n","    print(f\"\\nComparing cluster counts for {len(words)} wordplay terms\")\n","    print(f\"Models: {len(models)}\")\n","    print(f\"Cluster counts to test: {list(cluster_range)}\\n\")\n","\n","    # Initialize\n","    comparator = EmbeddingComparison(words)\n","\n","    # Run comparison\n","    df_results, csv_path, viz_path = comparator.compare_cluster_counts(\n","        model_names=models,\n","        cluster_range=cluster_range,\n","        reduction_method='umap'\n","    )\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"CLUSTER COUNT COMPARISON COMPLETE!\")\n","    print(\"=\"*70)\n","    print(f\"\\nResults saved to: {comparator.output_dir}/\")\n","    print(f\"  - Visualization: {viz_path.split('/')[-1]}\")\n","    print(f\"  - Data: {csv_path.split('/')[-1]}\")\n","\n","    # Print best configurations\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"Best Cluster Count per Model (by Silhouette Score):\")\n","    print(\"=\"*70)\n","\n","    for model in models:\n","        model_data = df_results[df_results['Model'] == model]\n","        best_idx = model_data['Silhouette'].idxmax()\n","        best_row = model_data.loc[best_idx]\n","\n","        print(f\"\\n{model.split('/')[-1]}:\")\n","        print(f\"  Optimal clusters: {int(best_row['N_Clusters'])}\")\n","        print(f\"  Silhouette: {best_row['Silhouette']:.4f}\")\n","        print(f\"  Davies-Bouldin: {best_row['Davies_Bouldin']:.4f}\")\n","\n","    return comparator, df_results\n","\n","\n","def run_full_optimal_analysis(words, cluster_range=None, models=None,\n","                              methods=['kmeans', 'dbscan', 'hierarchical']):\n","    \"\"\"\n","    Complete analysis: find optimal clusters and export members for all methods\n","\n","    This is the recommended function for comprehensive analysis!\n","\n","    Args:\n","        words: List of wordplay terms\n","        cluster_range: Range of cluster counts to test for kmeans/hierarchical\n","        models: List of model names to compare (uses defaults if None)\n","        methods: Clustering methods to try (default: all three)\n","\n","    Returns:\n","        EmbeddingComparison object and optimal results dictionary\n","    \"\"\"\n","\n","    # Default models\n","    if models is None:\n","        models = [\n","            'all-MiniLM-L6-v2',    # Fast\n","            'all-mpnet-base-v2',   # High quality\n","        ]\n","\n","    # Default cluster range\n","    if cluster_range is None:\n","        cluster_range = range(5, 31, 5)\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"COMPREHENSIVE OPTIMAL CLUSTERING ANALYSIS\")\n","    print(\"=\"*70)\n","    print(f\"\\nWords: {len(words)}\")\n","    print(f\"Models: {', '.join(models)}\")\n","    print(f\"Methods: {', '.join(methods)}\")\n","    print(f\"Cluster range: {list(cluster_range)}\")\n","    print(\"\\nThis will:\")\n","    print(\"  1. Test each model with multiple clustering methods\")\n","    print(\"  2. Find optimal parameters for each method\")\n","    print(\"  3. Export cluster members to CSV and text files\")\n","    print(\"  4. Create dendrograms for hierarchical clustering\")\n","    print(\"  5. Generate comparison visualizations\")\n","\n","    # Initialize\n","    comparator = EmbeddingComparison(words)\n","\n","    # Run comprehensive analysis\n","    optimal_results = comparator.find_and_export_optimal_clusters(\n","        model_names=models,\n","        cluster_range=cluster_range,\n","        methods=methods\n","    )\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"ANALYSIS COMPLETE! Generated files:\")\n","    print(\"=\"*70)\n","\n","    for model_name, method_results in optimal_results.items():\n","        print(f\"\\n{model_name}:\")\n","        for method, results in method_results.items():\n","            print(f\"  {method.upper()}:\")\n","            print(f\"    - {results['txt_path'].split('/')[-1]} (summary)\")\n","            print(f\"    - {results['csv_path'].split('/')[-1]} (full data)\")\n","            if 'dendrogram_path' in results:\n","                print(f\"    - {results['dendrogram_path'].split('/')[-1]} (dendrogram)\")\n","\n","    print(f\"\\n  Summary: optimal_clusters_summary.txt\")\n","\n","    return comparator, optimal_results\n","\n","\n","def compare_clustering_methods(words, cluster_range=None, model='all-mpnet-base-v2',\n","                               methods=['kmeans', 'hierarchical', 'dbscan']):\n","    \"\"\"\n","    Compare different clustering methods across cluster counts for a single model\n","\n","    Generates a plot showing how silhouette scores vary with cluster count\n","    for each clustering method.\n","\n","    Args:\n","        words: List of wordplay terms\n","        cluster_range: Range of cluster counts to test (default: 5 to 30 by 5)\n","        model: Which embedding model to use\n","        methods: Which clustering methods to compare\n","\n","    Returns:\n","        EmbeddingComparison object and results DataFrame\n","    \"\"\"\n","\n","    if cluster_range is None:\n","        cluster_range = range(5, 31, 5)\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"CLUSTERING METHOD COMPARISON\")\n","    print(\"=\"*70)\n","    print(f\"\\nModel: {model}\")\n","    print(f\"Methods: {', '.join(methods)}\")\n","    print(f\"Cluster range: {list(cluster_range)}\")\n","    print(\"\\nThis will generate:\")\n","    print(\"  â€¢ 2-panel plot comparing methods\")\n","    print(\"  â€¢ CSV with all metrics\")\n","\n","    # Initialize\n","    comparator = EmbeddingComparison(words)\n","\n","    # Run comparison\n","    df_results, csv_path, viz_path = comparator.compare_methods_across_clusters(\n","        model_name=model,\n","        cluster_range=cluster_range,\n","        methods=methods\n","    )\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"METHOD COMPARISON COMPLETE!\")\n","    print(\"=\"*70)\n","    print(f\"\\nResults:\")\n","    print(f\"  Visualization: {viz_path.split('/')[-1]}\")\n","    print(f\"  Data: {csv_path.split('/')[-1]}\")\n","\n","    # Print summary\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"Best Performance by Method:\")\n","    print(\"=\"*70)\n","\n","    for method in methods:\n","        method_data = df_results[df_results['Method'] == method]\n","\n","        if len(method_data) == 0:\n","            continue\n","\n","        best_idx = method_data['Silhouette'].idxmax()\n","        best_row = method_data.loc[best_idx]\n","\n","        print(f\"\\n{method.upper()}:\")\n","        if method == 'dbscan':\n","            print(f\"  Clusters found: {int(best_row['N_Clusters'])}\")\n","        else:\n","            print(f\"  Optimal clusters: {int(best_row['N_Clusters'])}\")\n","        print(f\"  Silhouette: {best_row['Silhouette']:.4f}\")\n","        print(f\"  Davies-Bouldin: {best_row['Davies_Bouldin']:.4f}\")\n","\n","    return comparator, df_results\n","\n","\n","# EXAMPLE USAGE OPTIONS:\n","\n","# Option 1: Load from file\n","# words = load_words_from_file('/mnt/user-data/uploads/your_wordplay_terms.txt')\n","# run_comparison(words, n_clusters=15)\n","words = load_words_from_file(IND_DIR)\n","\n","# Option 3: Custom models\n","# custom_models = [\n","#     'all-MiniLM-L6-v2',\n","#     'paraphrase-multilingual-mpnet-base-v2',\n","#     'sentence-transformers/all-roberta-large-v1'\n","# ]\n","# run_comparison(words, n_clusters=12, models=custom_models)\n","\n","# Option 4: Compare across cluster counts\n","# words = load_words_from_file('/mnt/user-data/uploads/your_wordplay_terms.txt')\n","# comparator, results_df = run_cluster_count_comparison(\n","#     words,\n","#     cluster_range=range(5, 51, 5),  # Test 5, 10, 15, ... 50\n","#     models=['all-MiniLM-L6-v2', 'all-mpnet-base-v2']\n","# )\n","\n","# Option 5: COMPREHENSIVE ANALYSIS (RECOMMENDED!)\n","# This finds optimal clusters and exports members for all methods\n","# words = load_words_from_file('/mnt/user-data/uploads/your_wordplay_terms.txt')\n","comparator, optimal_results = run_full_optimal_analysis(\n","    words,\n","    cluster_range=range(10, 51, 5),\n","    models=[#'all-MiniLM-L6-v2',\n","            'all-mpnet-base-v2'\n","            ],\n","    methods=[#'kmeans',\n","             #'dbscan',\n","             'hdbscan'\n","             #'hierarchical'\n","             ]\n",")\n","\n","# Option 6: COMPARE CLUSTERING METHODS (NEW!)\n","# Compare K-means vs Hierarchical vs DBSCAN on one model\n","# words = load_words_from_file('/mnt/user-data/uploads/your_wordplay_terms.txt')\n","#comparator, results_df = compare_clustering_methods(\n","#    words,\n","#   cluster_range=range(5, 51, 5),\n","#    model='all-mpnet-base-v2',\n","#    methods=['kmeans',\n","#             'hierarchical',\n","#             'dbscan'\n","#             ]\n","#)\n","\n","if __name__ == \"__main__\":\n","    print(\"=\"*70)\n","    print(\"EMBEDDING MODEL COMPARISON FOR WORDPLAY CLUSTERING\")\n","    print(\"=\"*70)\n","    print(\"\\nTo use this script:\")\n","    print(\"2. Uncomment and modify one of the example usage options above\")\n","    print(\"3. Run the script\")\n","    print(\"=\"*70)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01b37d4cf22946479ee77285abe7b1f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c5ece625a0245b89505f2490d802767","max":444,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef81235787914e28a22f7f717fe37996","value":444}},"2717d2ccc57a43efb1b9ef26faeaaa78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40ffa2585053449ebde664705893720e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"443a07893fef4d51b79310e43b01e9b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53c21d1303ce466ba6d9e2c91ccb42f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54c5c3ab55084506b682a33fe72ea7a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82273124bc4b4058b55bfaaf040626b3","max":199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b302b308ff742259faffe35e1d2a6d9","value":199}},"5c5ece625a0245b89505f2490d802767":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62bf95cf11b3402db87bfc842caef361":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_829ac1e784114ef5b2227f541647b6b3","placeholder":"â€‹","style":"IPY_MODEL_a3ea798ad8e241bea9740a4384f8e401","value":"Loadingâ€‡weights:â€‡100%"}},"7b302b308ff742259faffe35e1d2a6d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cdedcc52d3b47009648c6e6275552f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_443a07893fef4d51b79310e43b01e9b9","placeholder":"â€‹","style":"IPY_MODEL_7e217ead9ee34049975b5a02ef6ac30c","value":"â€‡444/444â€‡[05:45&lt;00:00,â€‡â€‡1.62it/s]"}},"7e217ead9ee34049975b5a02ef6ac30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82273124bc4b4058b55bfaaf040626b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"829ac1e784114ef5b2227f541647b6b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96e1f485da69478db85d0f162780bb3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2717d2ccc57a43efb1b9ef26faeaaa78","placeholder":"â€‹","style":"IPY_MODEL_40ffa2585053449ebde664705893720e","value":"Batches:â€‡100%"}},"a043bfe08440462d951f3534bc7910b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3ea798ad8e241bea9740a4384f8e401":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b08fc06b681642d3b45537d627932ea4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53c21d1303ce466ba6d9e2c91ccb42f1","placeholder":"â€‹","style":"IPY_MODEL_a043bfe08440462d951f3534bc7910b1","value":"â€‡199/199â€‡[00:00&lt;00:00,â€‡548.58it/s,â€‡Materializingâ€‡param=pooler.dense.weight]"}},"ca8aa85c5c0b4210be38aa75f3d6df66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62bf95cf11b3402db87bfc842caef361","IPY_MODEL_54c5c3ab55084506b682a33fe72ea7a6","IPY_MODEL_b08fc06b681642d3b45537d627932ea4"],"layout":"IPY_MODEL_f464257a85c64ad5bfe8ca7059aa63c4"}},"cc0e9b541ea9452dad5424027abff7a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef81235787914e28a22f7f717fe37996":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f464257a85c64ad5bfe8ca7059aa63c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f535213759bd4f5e80863b21cdcf63d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96e1f485da69478db85d0f162780bb3c","IPY_MODEL_01b37d4cf22946479ee77285abe7b1f4","IPY_MODEL_7cdedcc52d3b47009648c6e6275552f0"],"layout":"IPY_MODEL_cc0e9b541ea9452dad5424027abff7a3"}}}}},"nbformat":4,"nbformat_minor":0}